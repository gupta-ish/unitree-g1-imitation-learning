2025-10-27 13:16:34,059 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 13:16:34,059 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 13:16:34,059 - INFO - Using device: cuda
2025-10-27 13:16:34,073 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 13:16:34,073 - INFO - CUDA version: 12.6
2025-10-27 13:16:34,073 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 13:16:34,073 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets
2025-10-27 13:16:34,187 - ERROR - Training failed with error: 401 Client Error. (Request ID: Root=1-68ffa8f2-76a883767050160274a033f3;69139141-7103-4172-8efb-2018638dcc69)

Repository Not Found for url: https://huggingface.co/api/datasets/your_name/g1_dataset/refs.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 94, in __init__
    self.load_metadata()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 104, in load_metadata
    self.info = load_info(self.root)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 177, in load_info
    info = load_json(local_dir / INFO_PATH)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 145, in load_json
    with open(fpath) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/ishitagupta/zenavatar/lerobot_datasets/meta/info.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 407, in hf_raise_for_status
    response.raise_for_status()
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/datasets/your_name/g1_dataset/refs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 297, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 173, in train
    dataset = load_dataset()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 77, in load_dataset
    dataset = LeRobotDataset(
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 468, in __init__
    self.meta = LeRobotDatasetMetadata(
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 97, in __init__
    self.revision = get_safe_version(self.repo_id, self.revision)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 326, in get_safe_version
    hub_versions = get_repo_versions(repo_id)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 308, in get_repo_versions
    repo_refs = api.list_repo_refs(repo_id, repo_type="dataset")
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 3248, in list_repo_refs
    hf_raise_for_status(response)
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 457, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-68ffa8f2-76a883767050160274a033f3;69139141-7103-4172-8efb-2018638dcc69)

Repository Not Found for url: https://huggingface.co/api/datasets/your_name/g1_dataset/refs.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.
2025-10-27 13:18:15,427 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 13:18:15,427 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 13:18:15,427 - INFO - Using device: cuda
2025-10-27 13:18:15,447 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 13:18:15,447 - INFO - CUDA version: 12.6
2025-10-27 13:18:15,447 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 13:18:15,447 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets
2025-10-27 13:18:15,601 - ERROR - Training failed with error: 401 Client Error. (Request ID: Root=1-68ffa957-442eb99f29e9cfdb00d7eaee;ff8081ac-8bfc-4307-a663-6d7c1d864caa)

Repository Not Found for url: https://huggingface.co/api/datasets/your_name/g1_dataset/refs.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 94, in __init__
    self.load_metadata()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 104, in load_metadata
    self.info = load_info(self.root)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 177, in load_info
    info = load_json(local_dir / INFO_PATH)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 145, in load_json
    with open(fpath) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/ishitagupta/zenavatar/lerobot_datasets/meta/info.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 407, in hf_raise_for_status
    response.raise_for_status()
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/datasets/your_name/g1_dataset/refs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 297, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 173, in train
    dataset = load_dataset()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 77, in load_dataset
    dataset = LeRobotDataset(
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 468, in __init__
    self.meta = LeRobotDatasetMetadata(
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 97, in __init__
    self.revision = get_safe_version(self.repo_id, self.revision)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 326, in get_safe_version
    hub_versions = get_repo_versions(repo_id)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/utils.py", line 308, in get_repo_versions
    repo_refs = api.list_repo_refs(repo_id, repo_type="dataset")
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 3248, in list_repo_refs
    hf_raise_for_status(response)
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 457, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-68ffa957-442eb99f29e9cfdb00d7eaee;ff8081ac-8bfc-4307-a663-6d7c1d864caa)

Repository Not Found for url: https://huggingface.co/api/datasets/your_name/g1_dataset/refs.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.
2025-10-27 13:19:22,325 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 13:19:22,325 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 13:19:22,325 - INFO - Using device: cuda
2025-10-27 13:19:22,344 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 13:19:22,344 - INFO - CUDA version: 12.6
2025-10-27 13:19:22,344 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 13:19:22,344 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 13:19:23,328 - INFO - Dataset loaded successfully
2025-10-27 13:19:23,329 - INFO - Number of episodes: 150
2025-10-27 13:19:23,329 - INFO - Number of frames: 93464
2025-10-27 13:19:23,329 - INFO - FPS: 30
2025-10-27 13:19:23,329 - INFO - Creating diffusion policy...
2025-10-27 13:19:23,329 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 13:19:23,329 - INFO - Output features: ['action']
2025-10-27 13:19:23,329 - INFO - Cuda backend detected, using cuda.
2025-10-27 13:19:23,329 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 13:19:24,367 - INFO - Total parameters: 263,440,264
2025-10-27 13:19:24,367 - INFO - Trainable parameters: 263,440,174
2025-10-27 13:19:24,367 - INFO - Creating dataloader...
2025-10-27 13:19:24,367 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 13:19:24,367 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 13:19:24,367 - INFO - ================================================================================
2025-10-27 13:19:24,367 - INFO - Starting training
2025-10-27 13:19:24,367 - INFO - ================================================================================
2025-10-27 13:19:24,416 - ERROR - Training failed with error: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 720, in __getitem__
    video_frames = self._query_videos(query_timestamps, ep_idx)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 692, in _query_videos
    frames = decode_video_frames(video_path, query_ts, self.tolerance_s, self.video_backend)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/video_utils.py", line 66, in decode_video_frames
    return decode_video_frames_torchcodec(video_path, timestamps, tolerance_s)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/video_utils.py", line 190, in decode_video_frames_torchcodec
    from torchcodec.decoders import VideoDecoder
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/__init__.py", line 10, in <module>
    from . import decoders, samplers  # noqa
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/decoders/__init__.py", line 7, in <module>
    from .._core import AudioStreamMetadata, VideoStreamMetadata
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/__init__.py", line 8, in <module>
    from ._metadata import (
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/_metadata.py", line 16, in <module>
    from torchcodec._core.ops import (
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/ops.py", line 84, in <module>
    load_torchcodec_shared_libraries()
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/ops.py", line 69, in load_torchcodec_shared_libraries
    raise RuntimeError(
RuntimeError: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6 and 7.
          2. The PyTorch version (2.7.1+cu126) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: libavutil.so.58: cannot open shared object file: No such file or directory
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: libavutil.so.56: cannot open shared object file: No such file or directory
[end of libtorchcodec loading traceback].
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 297, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 203, in train
    batch = next(dataloader_iter)
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 733, in __next__
    data = self._next_data()
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1515, in _next_data
    return self._process_data(data, worker_id)
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1550, in _process_data
    data.reraise()
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/_utils.py", line 750, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 720, in __getitem__
    video_frames = self._query_videos(query_timestamps, ep_idx)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/lerobot_dataset.py", line 692, in _query_videos
    frames = decode_video_frames(video_path, query_ts, self.tolerance_s, self.video_backend)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/video_utils.py", line 66, in decode_video_frames
    return decode_video_frames_torchcodec(video_path, timestamps, tolerance_s)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/datasets/video_utils.py", line 190, in decode_video_frames_torchcodec
    from torchcodec.decoders import VideoDecoder
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/__init__.py", line 10, in <module>
    from . import decoders, samplers  # noqa
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/decoders/__init__.py", line 7, in <module>
    from .._core import AudioStreamMetadata, VideoStreamMetadata
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/__init__.py", line 8, in <module>
    from ._metadata import (
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/_metadata.py", line 16, in <module>
    from torchcodec._core.ops import (
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/ops.py", line 84, in <module>
    load_torchcodec_shared_libraries()
  File "/home/ishitagupta/miniforge3/envs/unitree_IL/lib/python3.10/site-packages/torchcodec/_core/ops.py", line 69, in load_torchcodec_shared_libraries
    raise RuntimeError(
RuntimeError: Could not load libtorchcodec. Likely causes:
          1. FFmpeg is not properly installed in your environment. We support
             versions 4, 5, 6 and 7.
          2. The PyTorch version (2.7.1+cu126) is not compatible with
             this version of TorchCodec. Refer to the version compatibility
             table:
             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.
          3. Another runtime dependency; see exceptions below.
        The following exceptions were raised as we tried to load libtorchcodec:
        
[start of libtorchcodec loading traceback]
FFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory
FFmpeg version 6: libavutil.so.58: cannot open shared object file: No such file or directory
FFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory
FFmpeg version 4: libavutil.so.56: cannot open shared object file: No such file or directory
[end of libtorchcodec loading traceback].

2025-10-27 13:20:14,331 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 13:20:14,331 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 13:20:14,331 - INFO - Using device: cuda
2025-10-27 13:20:14,347 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 13:20:14,347 - INFO - CUDA version: 12.6
2025-10-27 13:20:14,347 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 13:20:14,347 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 13:20:15,131 - INFO - Dataset loaded successfully
2025-10-27 13:20:15,131 - INFO - Number of episodes: 150
2025-10-27 13:20:15,131 - INFO - Number of frames: 93464
2025-10-27 13:20:15,131 - INFO - FPS: 30
2025-10-27 13:20:15,131 - INFO - Creating diffusion policy...
2025-10-27 13:20:15,131 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 13:20:15,131 - INFO - Output features: ['action']
2025-10-27 13:20:15,131 - INFO - Cuda backend detected, using cuda.
2025-10-27 13:20:15,131 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 13:20:16,142 - INFO - Total parameters: 263,440,264
2025-10-27 13:20:16,142 - INFO - Trainable parameters: 263,440,174
2025-10-27 13:20:16,142 - INFO - Creating dataloader...
2025-10-27 13:20:16,142 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 13:20:16,142 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 13:20:16,143 - INFO - ================================================================================
2025-10-27 13:20:16,143 - INFO - Starting training
2025-10-27 13:20:16,143 - INFO - ================================================================================
2025-10-27 13:20:16,398 - ERROR - Training failed with error: 
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 299, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 220, in train
    loss, _ = policy.forward(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 161, in forward
    loss = self.diffusion.compute_loss(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 325, in compute_loss
    assert set(batch).issuperset({"observation.state", "action", "action_is_pad"})
AssertionError
2025-10-27 13:21:12,373 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 13:21:12,373 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 13:21:12,373 - INFO - Using device: cuda
2025-10-27 13:21:12,391 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 13:21:12,391 - INFO - CUDA version: 12.6
2025-10-27 13:21:12,391 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 13:21:12,391 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 13:21:13,174 - INFO - Dataset loaded successfully
2025-10-27 13:21:13,174 - INFO - Number of episodes: 150
2025-10-27 13:21:13,175 - INFO - Number of frames: 93464
2025-10-27 13:21:13,175 - INFO - FPS: 30
2025-10-27 13:21:13,175 - INFO - Creating diffusion policy...
2025-10-27 13:21:13,175 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 13:21:13,175 - INFO - Output features: ['action']
2025-10-27 13:21:13,175 - INFO - Cuda backend detected, using cuda.
2025-10-27 13:21:13,175 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 13:21:14,187 - INFO - Total parameters: 263,440,264
2025-10-27 13:21:14,187 - INFO - Trainable parameters: 263,440,174
2025-10-27 13:21:14,187 - INFO - Creating dataloader...
2025-10-27 13:21:14,187 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 13:21:14,187 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 13:21:14,187 - INFO - ================================================================================
2025-10-27 13:21:14,187 - INFO - Starting training
2025-10-27 13:21:14,187 - INFO - ================================================================================
2025-10-27 13:21:14,437 - ERROR - Training failed with error: 
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 305, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 226, in train
    loss, _ = policy.forward(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 161, in forward
    loss = self.diffusion.compute_loss(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 329, in compute_loss
    assert horizon == self.config.horizon
AssertionError
2025-10-27 14:14:06,486 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 14:14:06,486 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 14:14:06,486 - INFO - Using device: cuda
2025-10-27 14:14:06,499 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 14:14:06,499 - INFO - CUDA version: 12.6
2025-10-27 14:14:06,499 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:14:06,499 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:14:07,315 - INFO - Dataset loaded successfully
2025-10-27 14:14:07,315 - INFO - Number of episodes: 150
2025-10-27 14:14:07,316 - INFO - Number of frames: 93464
2025-10-27 14:14:07,316 - INFO - FPS: 30
2025-10-27 14:14:07,316 - INFO - Creating diffusion policy...
2025-10-27 14:14:07,316 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 14:14:07,316 - INFO - Output features: ['action']
2025-10-27 14:14:07,316 - INFO - Cuda backend detected, using cuda.
2025-10-27 14:14:07,316 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 14:14:08,380 - INFO - Total parameters: 263,440,264
2025-10-27 14:14:08,380 - INFO - Trainable parameters: 263,440,174
2025-10-27 14:14:08,380 - INFO - Creating dataloader...
2025-10-27 14:14:08,380 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 14:14:08,380 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 14:14:08,381 - INFO - ================================================================================
2025-10-27 14:14:08,381 - INFO - Starting training
2025-10-27 14:14:08,381 - INFO - ================================================================================
2025-10-27 14:14:08,663 - ERROR - Training failed with error: 
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 305, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 226, in train
    loss, _ = policy.forward(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 161, in forward
    loss = self.diffusion.compute_loss(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 329, in compute_loss
    assert horizon == self.config.horizon
AssertionError
2025-10-27 14:16:10,069 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 14:16:10,069 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 14:16:10,069 - INFO - Using device: cuda
2025-10-27 14:16:10,081 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 14:16:10,081 - INFO - CUDA version: 12.6
2025-10-27 14:16:10,081 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:16:10,081 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:16:10,868 - INFO - Dataset loaded successfully
2025-10-27 14:16:10,868 - INFO - Number of episodes: 150
2025-10-27 14:16:10,868 - INFO - Number of frames: 93464
2025-10-27 14:16:10,868 - INFO - FPS: 30
2025-10-27 14:16:10,868 - INFO - Creating diffusion policy...
2025-10-27 14:16:10,868 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 14:16:10,868 - INFO - Output features: ['action']
2025-10-27 14:16:10,868 - INFO - Cuda backend detected, using cuda.
2025-10-27 14:16:10,868 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 14:16:11,928 - INFO - Total parameters: 263,440,264
2025-10-27 14:16:11,928 - INFO - Trainable parameters: 263,440,174
2025-10-27 14:16:11,928 - INFO - Creating dataloader...
2025-10-27 14:16:11,929 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 14:16:11,929 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 14:16:11,929 - INFO - ================================================================================
2025-10-27 14:16:11,929 - INFO - Starting training
2025-10-27 14:16:11,929 - INFO - ================================================================================
2025-10-27 14:16:12,109 - INFO - ================================================================================
2025-10-27 14:16:12,109 - INFO - Batch shapes (first iteration):
2025-10-27 14:16:12,109 - INFO -   observation.images.cam_left_high: torch.Size([8, 3, 480, 640])
2025-10-27 14:16:12,109 - INFO -   observation.state: torch.Size([8, 14])
2025-10-27 14:16:12,110 - INFO -   action: torch.Size([8, 14])
2025-10-27 14:16:12,110 - INFO -   timestamp: torch.Size([8])
2025-10-27 14:16:12,110 - INFO -   frame_index: torch.Size([8])
2025-10-27 14:16:12,110 - INFO -   episode_index: torch.Size([8])
2025-10-27 14:16:12,110 - INFO -   index: torch.Size([8])
2025-10-27 14:16:12,110 - INFO -   task_index: torch.Size([8])
2025-10-27 14:16:12,110 - INFO - Expected horizon: 16
2025-10-27 14:16:12,110 - INFO - Expected n_obs_steps: 2
2025-10-27 14:16:12,110 - INFO - Config action_delta_indices: [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
2025-10-27 14:16:12,110 - INFO - Config observation_delta_indices: [-1, 0]
2025-10-27 14:16:12,110 - INFO - ================================================================================
2025-10-27 14:16:12,169 - ERROR - Training failed with error: 
Traceback (most recent call last):
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 318, in <module>
    train()
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/train_g1_diffusion.py", line 239, in train
    loss, _ = policy.forward(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 161, in forward
    loss = self.diffusion.compute_loss(batch)
  File "/home/ishitagupta/zenavatar/unitree_IL_lerobot/unitree_lerobot/lerobot/src/lerobot/policies/diffusion/modeling_diffusion.py", line 329, in compute_loss
    assert horizon == self.config.horizon
AssertionError
2025-10-27 14:17:49,420 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 14:17:49,421 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 14:17:49,421 - INFO - Using device: cuda
2025-10-27 14:17:49,436 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 14:17:49,436 - INFO - CUDA version: 12.6
2025-10-27 14:17:49,436 - INFO - Loading dataset metadata...
2025-10-27 14:17:49,436 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:17:49,436 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:17:50,251 - INFO - Dataset loaded successfully
2025-10-27 14:17:50,251 - INFO - Number of episodes: 150
2025-10-27 14:17:50,251 - INFO - Number of frames: 93464
2025-10-27 14:17:50,251 - INFO - FPS: 30
2025-10-27 14:17:50,251 - INFO - Creating policy configuration...
2025-10-27 14:17:50,252 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 14:17:50,252 - INFO - Output features: ['action']
2025-10-27 14:17:50,252 - INFO - Cuda backend detected, using cuda.
2025-10-27 14:17:50,252 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 14:17:50,252 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 14:17:50,252 - INFO - Reloading dataset with temporal configuration...
2025-10-27 14:17:50,252 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:17:50,252 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:17:50,886 - INFO - Dataset loaded successfully
2025-10-27 14:17:50,886 - INFO - Number of episodes: 150
2025-10-27 14:17:50,886 - INFO - Number of frames: 93464
2025-10-27 14:17:50,886 - INFO - FPS: 30
2025-10-27 14:17:50,886 - INFO - Creating diffusion policy...
2025-10-27 14:17:51,957 - INFO - Total parameters: 263,440,264
2025-10-27 14:17:51,957 - INFO - Trainable parameters: 263,440,174
2025-10-27 14:17:51,957 - INFO - Creating dataloader...
2025-10-27 14:17:51,957 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 14:17:51,957 - INFO - ================================================================================
2025-10-27 14:17:51,957 - INFO - Starting training
2025-10-27 14:17:51,957 - INFO - ================================================================================
2025-10-27 14:17:58,370 - INFO - Step 100/100000 | Loss: 1.0028 | LR: 1.00e-04
2025-10-27 14:18:03,858 - INFO - Step 200/100000 | Loss: 0.3015 | LR: 1.00e-04
2025-10-27 14:18:08,573 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 14:18:08,573 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 14:18:08,573 - INFO - Using device: cuda
2025-10-27 14:18:08,590 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 14:18:08,590 - INFO - CUDA version: 12.6
2025-10-27 14:18:08,590 - INFO - Loading dataset metadata...
2025-10-27 14:18:08,590 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:18:08,590 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:18:09,443 - INFO - Step 300/100000 | Loss: 0.1246 | LR: 1.00e-04
2025-10-27 14:18:09,570 - INFO - Dataset loaded successfully
2025-10-27 14:18:09,570 - INFO - Number of episodes: 150
2025-10-27 14:18:09,570 - INFO - Number of frames: 93464
2025-10-27 14:18:09,570 - INFO - FPS: 30
2025-10-27 14:18:09,570 - INFO - Creating policy configuration...
2025-10-27 14:18:09,570 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 14:18:09,570 - INFO - Output features: ['action']
2025-10-27 14:18:09,570 - INFO - Cuda backend detected, using cuda.
2025-10-27 14:18:09,570 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 14:18:09,571 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 14:18:09,571 - INFO - Reloading dataset with temporal configuration...
2025-10-27 14:18:09,571 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:18:09,571 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:18:10,361 - INFO - Dataset loaded successfully
2025-10-27 14:18:10,361 - INFO - Number of episodes: 150
2025-10-27 14:18:10,361 - INFO - Number of frames: 93464
2025-10-27 14:18:10,361 - INFO - FPS: 30
2025-10-27 14:18:10,361 - INFO - Creating diffusion policy...
2025-10-27 14:18:11,802 - INFO - Total parameters: 263,440,264
2025-10-27 14:18:11,802 - INFO - Trainable parameters: 263,440,174
2025-10-27 14:18:11,802 - INFO - Creating dataloader...
2025-10-27 14:18:11,802 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 14:18:11,803 - INFO - ================================================================================
2025-10-27 14:18:11,803 - INFO - Starting training
2025-10-27 14:18:11,803 - INFO - ================================================================================
2025-10-27 14:18:17,058 - INFO - Step 400/100000 | Loss: 0.0939 | LR: 1.00e-04
2025-10-27 14:18:22,942 - INFO - Step 100/100000 | Loss: 1.0072 | LR: 1.00e-04
2025-10-27 14:18:27,175 - INFO - Step 500/100000 | Loss: 0.0793 | LR: 1.00e-04
2025-10-27 14:18:27,669 - INFO - 
Training interrupted by user
2025-10-27 14:18:32,842 - INFO - Step 600/100000 | Loss: 0.0756 | LR: 1.00e-04
2025-10-27 14:18:38,214 - INFO - Step 700/100000 | Loss: 0.0593 | LR: 1.00e-04
2025-10-27 14:18:43,721 - INFO - Step 800/100000 | Loss: 0.0561 | LR: 1.00e-04
2025-10-27 14:19:45,589 - INFO - Output directory: outputs/train/g1_diffusion
2025-10-27 14:19:45,589 - INFO - Checkpoint directory: outputs/train/g1_diffusion/checkpoints
2025-10-27 14:19:45,589 - INFO - Using device: cuda
2025-10-27 14:19:45,603 - INFO - GPU: NVIDIA GeForce RTX 4090
2025-10-27 14:19:45,603 - INFO - CUDA version: 12.6
2025-10-27 14:19:45,603 - INFO - Loading dataset metadata...
2025-10-27 14:19:45,603 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:19:45,603 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:19:46,388 - INFO - Dataset loaded successfully
2025-10-27 14:19:46,389 - INFO - Number of episodes: 150
2025-10-27 14:19:46,389 - INFO - Number of frames: 93464
2025-10-27 14:19:46,389 - INFO - FPS: 30
2025-10-27 14:19:46,389 - INFO - Creating policy configuration...
2025-10-27 14:19:46,389 - INFO - Input features: ['observation.state', 'observation.images.cam_left_high']
2025-10-27 14:19:46,389 - INFO - Output features: ['action']
2025-10-27 14:19:46,389 - INFO - Cuda backend detected, using cuda.
2025-10-27 14:19:46,389 - WARNING - Device 'None' is not available. Switching to 'cuda'.
2025-10-27 14:19:46,389 - INFO - Delta timestamps: {'observation.state': [-0.03333333333333333, 0.0], 'action': [-0.03333333333333333, 0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667], 'observation.images.cam_left_high': [-0.03333333333333333, 0.0]}
2025-10-27 14:19:46,389 - INFO - Reloading dataset with temporal configuration...
2025-10-27 14:19:46,389 - INFO - Loading dataset: your_name/g1_dataset
2025-10-27 14:19:46,389 - INFO - Dataset root: /home/ishitagupta/zenavatar/lerobot_datasets/your_name/g1_dataset
2025-10-27 14:19:47,054 - INFO - Dataset loaded successfully
2025-10-27 14:19:47,054 - INFO - Number of episodes: 150
2025-10-27 14:19:47,054 - INFO - Number of frames: 93464
2025-10-27 14:19:47,054 - INFO - FPS: 30
2025-10-27 14:19:47,054 - INFO - Creating diffusion policy...
2025-10-27 14:19:48,112 - INFO - Total parameters: 263,440,264
2025-10-27 14:19:48,112 - INFO - Trainable parameters: 263,440,174
2025-10-27 14:19:48,112 - INFO - Creating dataloader...
2025-10-27 14:19:48,112 - INFO - Dataloader created with batch_size=8, num_workers=4
2025-10-27 14:19:48,113 - INFO - ================================================================================
2025-10-27 14:19:48,113 - INFO - Starting training
2025-10-27 14:19:48,113 - INFO - ================================================================================
2025-10-27 14:19:54,232 - INFO - Step 100/100000 | Loss: 1.0042 | LR: 1.00e-04
2025-10-27 14:19:59,597 - INFO - Step 200/100000 | Loss: 0.3011 | LR: 1.00e-04
2025-10-27 14:20:04,943 - INFO - Step 300/100000 | Loss: 0.1281 | LR: 1.00e-04
2025-10-27 14:20:10,266 - INFO - Step 400/100000 | Loss: 0.0900 | LR: 1.00e-04
2025-10-27 14:20:15,597 - INFO - Step 500/100000 | Loss: 0.0712 | LR: 1.00e-04
2025-10-27 14:20:20,948 - INFO - Step 600/100000 | Loss: 0.0646 | LR: 1.00e-04
2025-10-27 14:20:26,313 - INFO - Step 700/100000 | Loss: 0.0624 | LR: 1.00e-04
2025-10-27 14:20:31,657 - INFO - Step 800/100000 | Loss: 0.0553 | LR: 1.00e-04
2025-10-27 14:20:37,049 - INFO - Step 900/100000 | Loss: 0.0509 | LR: 1.00e-04
2025-10-27 14:20:42,430 - INFO - Step 1000/100000 | Loss: 0.0480 | LR: 1.00e-04
2025-10-27 14:20:47,800 - INFO - Step 1100/100000 | Loss: 0.0480 | LR: 1.00e-04
2025-10-27 14:20:53,165 - INFO - Step 1200/100000 | Loss: 0.0472 | LR: 1.00e-04
2025-10-27 14:20:58,553 - INFO - Step 1300/100000 | Loss: 0.0372 | LR: 1.00e-04
2025-10-27 14:21:03,858 - INFO - Step 1400/100000 | Loss: 0.0448 | LR: 1.00e-04
2025-10-27 14:21:09,198 - INFO - Step 1500/100000 | Loss: 0.0382 | LR: 9.99e-05
2025-10-27 14:21:14,553 - INFO - Step 1600/100000 | Loss: 0.0380 | LR: 9.99e-05
2025-10-27 14:21:19,856 - INFO - Step 1700/100000 | Loss: 0.0351 | LR: 9.99e-05
2025-10-27 14:21:25,205 - INFO - Step 1800/100000 | Loss: 0.0337 | LR: 9.99e-05
2025-10-27 14:21:30,584 - INFO - Step 1900/100000 | Loss: 0.0363 | LR: 9.99e-05
2025-10-27 14:21:35,988 - INFO - Step 2000/100000 | Loss: 0.0371 | LR: 9.99e-05
2025-10-27 14:21:41,372 - INFO - Step 2100/100000 | Loss: 0.0347 | LR: 9.99e-05
2025-10-27 14:21:46,737 - INFO - Step 2200/100000 | Loss: 0.0321 | LR: 9.99e-05
2025-10-27 14:21:52,120 - INFO - Step 2300/100000 | Loss: 0.0306 | LR: 9.99e-05
2025-10-27 14:21:57,479 - INFO - Step 2400/100000 | Loss: 0.0335 | LR: 9.99e-05
2025-10-27 14:22:02,910 - INFO - Step 2500/100000 | Loss: 0.0316 | LR: 9.99e-05
2025-10-27 14:22:08,252 - INFO - Step 2600/100000 | Loss: 0.0315 | LR: 9.98e-05
2025-10-27 14:22:13,559 - INFO - Step 2700/100000 | Loss: 0.0318 | LR: 9.98e-05
2025-10-27 14:22:18,884 - INFO - Step 2800/100000 | Loss: 0.0302 | LR: 9.98e-05
2025-10-27 14:22:24,222 - INFO - Step 2900/100000 | Loss: 0.0306 | LR: 9.98e-05
2025-10-27 14:22:29,609 - INFO - Step 3000/100000 | Loss: 0.0311 | LR: 9.98e-05
2025-10-27 14:22:34,992 - INFO - Step 3100/100000 | Loss: 0.0287 | LR: 9.98e-05
2025-10-27 14:22:40,282 - INFO - Step 3200/100000 | Loss: 0.0332 | LR: 9.98e-05
2025-10-27 14:22:45,581 - INFO - Step 3300/100000 | Loss: 0.0287 | LR: 9.98e-05
2025-10-27 14:22:50,888 - INFO - Step 3400/100000 | Loss: 0.0256 | LR: 9.97e-05
2025-10-27 14:22:56,246 - INFO - Step 3500/100000 | Loss: 0.0259 | LR: 9.97e-05
2025-10-27 14:23:01,610 - INFO - Step 3600/100000 | Loss: 0.0300 | LR: 9.97e-05
2025-10-27 14:23:06,982 - INFO - Step 3700/100000 | Loss: 0.0271 | LR: 9.97e-05
2025-10-27 14:23:12,352 - INFO - Step 3800/100000 | Loss: 0.0287 | LR: 9.97e-05
2025-10-27 14:23:17,707 - INFO - Step 3900/100000 | Loss: 0.0306 | LR: 9.97e-05
2025-10-27 14:23:22,997 - INFO - Step 4000/100000 | Loss: 0.0267 | LR: 9.96e-05
2025-10-27 14:23:28,311 - INFO - Step 4100/100000 | Loss: 0.0273 | LR: 9.96e-05
2025-10-27 14:23:33,665 - INFO - Step 4200/100000 | Loss: 0.0289 | LR: 9.96e-05
2025-10-27 14:23:38,991 - INFO - Step 4300/100000 | Loss: 0.0255 | LR: 9.96e-05
2025-10-27 14:23:44,283 - INFO - Step 4400/100000 | Loss: 0.0259 | LR: 9.96e-05
2025-10-27 14:23:49,618 - INFO - Step 4500/100000 | Loss: 0.0276 | LR: 9.96e-05
2025-10-27 14:23:54,963 - INFO - Step 4600/100000 | Loss: 0.0259 | LR: 9.95e-05
2025-10-27 14:24:00,306 - INFO - Step 4700/100000 | Loss: 0.0253 | LR: 9.95e-05
2025-10-27 14:24:05,650 - INFO - Step 4800/100000 | Loss: 0.0225 | LR: 9.95e-05
2025-10-27 14:24:10,923 - INFO - Step 4900/100000 | Loss: 0.0262 | LR: 9.95e-05
2025-10-27 14:24:16,269 - INFO - Step 5000/100000 | Loss: 0.0253 | LR: 9.94e-05
2025-10-27 14:24:21,658 - INFO - Step 5100/100000 | Loss: 0.0269 | LR: 9.94e-05
2025-10-27 14:24:27,026 - INFO - Step 5200/100000 | Loss: 0.0245 | LR: 9.94e-05
2025-10-27 14:24:32,411 - INFO - Step 5300/100000 | Loss: 0.0261 | LR: 9.94e-05
2025-10-27 14:24:37,798 - INFO - Step 5400/100000 | Loss: 0.0259 | LR: 9.94e-05
2025-10-27 14:24:43,299 - INFO - Step 5500/100000 | Loss: 0.0244 | LR: 9.93e-05
2025-10-27 14:24:48,806 - INFO - Step 5600/100000 | Loss: 0.0249 | LR: 9.93e-05
2025-10-27 14:24:54,362 - INFO - Step 5700/100000 | Loss: 0.0252 | LR: 9.93e-05
2025-10-27 14:24:59,869 - INFO - Step 5800/100000 | Loss: 0.0238 | LR: 9.93e-05
2025-10-27 14:25:05,410 - INFO - Step 5900/100000 | Loss: 0.0245 | LR: 9.92e-05
2025-10-27 14:25:10,945 - INFO - Step 6000/100000 | Loss: 0.0210 | LR: 9.92e-05
2025-10-27 14:25:16,467 - INFO - Step 6100/100000 | Loss: 0.0216 | LR: 9.92e-05
2025-10-27 14:25:21,959 - INFO - Step 6200/100000 | Loss: 0.0252 | LR: 9.91e-05
2025-10-27 14:25:27,454 - INFO - Step 6300/100000 | Loss: 0.0194 | LR: 9.91e-05
2025-10-27 14:25:33,082 - INFO - Step 6400/100000 | Loss: 0.0237 | LR: 9.91e-05
2025-10-27 14:25:38,616 - INFO - Step 6500/100000 | Loss: 0.0257 | LR: 9.91e-05
2025-10-27 14:25:44,256 - INFO - Step 6600/100000 | Loss: 0.0232 | LR: 9.90e-05
2025-10-27 14:25:49,957 - INFO - Step 6700/100000 | Loss: 0.0246 | LR: 9.90e-05
2025-10-27 14:25:55,466 - INFO - Step 6800/100000 | Loss: 0.0238 | LR: 9.90e-05
2025-10-27 14:26:01,008 - INFO - Step 6900/100000 | Loss: 0.0234 | LR: 9.89e-05
2025-10-27 14:26:06,625 - INFO - Step 7000/100000 | Loss: 0.0221 | LR: 9.89e-05
2025-10-27 14:26:12,272 - INFO - Step 7100/100000 | Loss: 0.0243 | LR: 9.89e-05
2025-10-27 14:26:17,908 - INFO - Step 7200/100000 | Loss: 0.0214 | LR: 9.89e-05
2025-10-27 14:26:25,094 - INFO - Step 7300/100000 | Loss: 0.0198 | LR: 9.88e-05
2025-10-27 14:26:30,944 - INFO - Step 7400/100000 | Loss: 0.0220 | LR: 9.88e-05
2025-10-27 14:26:36,915 - INFO - Step 7500/100000 | Loss: 0.0237 | LR: 9.88e-05
2025-10-27 14:26:42,840 - INFO - Step 7600/100000 | Loss: 0.0189 | LR: 9.87e-05
2025-10-27 14:26:48,856 - INFO - Step 7700/100000 | Loss: 0.0210 | LR: 9.87e-05
2025-10-27 14:26:54,869 - INFO - Step 7800/100000 | Loss: 0.0243 | LR: 9.87e-05
2025-10-27 14:27:00,853 - INFO - Step 7900/100000 | Loss: 0.0228 | LR: 9.86e-05
2025-10-27 14:27:06,898 - INFO - Step 8000/100000 | Loss: 0.0200 | LR: 9.86e-05
2025-10-27 14:27:12,893 - INFO - Step 8100/100000 | Loss: 0.0203 | LR: 9.86e-05
2025-10-27 14:27:18,888 - INFO - Step 8200/100000 | Loss: 0.0212 | LR: 9.85e-05
2025-10-27 14:27:24,916 - INFO - Step 8300/100000 | Loss: 0.0240 | LR: 9.85e-05
2025-10-27 14:27:30,924 - INFO - Step 8400/100000 | Loss: 0.0213 | LR: 9.84e-05
2025-10-27 14:27:36,932 - INFO - Step 8500/100000 | Loss: 0.0196 | LR: 9.84e-05
2025-10-27 14:27:42,931 - INFO - Step 8600/100000 | Loss: 0.0217 | LR: 9.84e-05
2025-10-27 14:27:48,927 - INFO - Step 8700/100000 | Loss: 0.0225 | LR: 9.83e-05
2025-10-27 14:27:54,944 - INFO - Step 8800/100000 | Loss: 0.0194 | LR: 9.83e-05
2025-10-27 14:28:00,975 - INFO - Step 8900/100000 | Loss: 0.0199 | LR: 9.83e-05
2025-10-27 14:28:06,996 - INFO - Step 9000/100000 | Loss: 0.0192 | LR: 9.82e-05
2025-10-27 14:28:13,041 - INFO - Step 9100/100000 | Loss: 0.0180 | LR: 9.82e-05
2025-10-27 14:28:19,140 - INFO - Step 9200/100000 | Loss: 0.0218 | LR: 9.81e-05
2025-10-27 14:28:25,217 - INFO - Step 9300/100000 | Loss: 0.0199 | LR: 9.81e-05
2025-10-27 14:28:31,237 - INFO - Step 9400/100000 | Loss: 0.0202 | LR: 9.81e-05
2025-10-27 14:28:37,279 - INFO - Step 9500/100000 | Loss: 0.0195 | LR: 9.80e-05
2025-10-27 14:28:43,312 - INFO - Step 9600/100000 | Loss: 0.0217 | LR: 9.80e-05
2025-10-27 14:28:49,356 - INFO - Step 9700/100000 | Loss: 0.0207 | LR: 9.79e-05
2025-10-27 14:28:55,385 - INFO - Step 9800/100000 | Loss: 0.0208 | LR: 9.79e-05
2025-10-27 14:29:01,420 - INFO - Step 9900/100000 | Loss: 0.0192 | LR: 9.78e-05
2025-10-27 14:29:07,467 - INFO - Step 10000/100000 | Loss: 0.0198 | LR: 9.78e-05
2025-10-27 14:29:07,468 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_10000
2025-10-27 14:29:09,907 - INFO - New best loss: 0.0074, saving to outputs/train/g1_diffusion/checkpoints/best
2025-10-27 14:29:16,531 - INFO - Step 10100/100000 | Loss: 0.0169 | LR: 9.78e-05
2025-10-27 14:29:22,018 - INFO - Step 10200/100000 | Loss: 0.0173 | LR: 9.77e-05
2025-10-27 14:29:27,494 - INFO - Step 10300/100000 | Loss: 0.0191 | LR: 9.77e-05
2025-10-27 14:29:33,023 - INFO - Step 10400/100000 | Loss: 0.0228 | LR: 9.76e-05
2025-10-27 14:29:38,544 - INFO - Step 10500/100000 | Loss: 0.0201 | LR: 9.76e-05
2025-10-27 14:29:43,985 - INFO - Step 10600/100000 | Loss: 0.0192 | LR: 9.75e-05
2025-10-27 14:29:49,390 - INFO - Step 10700/100000 | Loss: 0.0167 | LR: 9.75e-05
2025-10-27 14:29:54,793 - INFO - Step 10800/100000 | Loss: 0.0191 | LR: 9.74e-05
2025-10-27 14:30:00,222 - INFO - Step 10900/100000 | Loss: 0.0225 | LR: 9.74e-05
2025-10-27 14:30:05,669 - INFO - Step 11000/100000 | Loss: 0.0199 | LR: 9.73e-05
2025-10-27 14:30:11,027 - INFO - Step 11100/100000 | Loss: 0.0155 | LR: 9.73e-05
2025-10-27 14:30:16,480 - INFO - Step 11200/100000 | Loss: 0.0171 | LR: 9.72e-05
2025-10-27 14:30:21,947 - INFO - Step 11300/100000 | Loss: 0.0163 | LR: 9.72e-05
2025-10-27 14:30:27,408 - INFO - Step 11400/100000 | Loss: 0.0183 | LR: 9.71e-05
2025-10-27 14:30:34,662 - INFO - Step 11500/100000 | Loss: 0.0173 | LR: 9.71e-05
2025-10-27 14:30:40,191 - INFO - Step 11600/100000 | Loss: 0.0198 | LR: 9.70e-05
2025-10-27 14:30:45,506 - INFO - Completed epoch 1
2025-10-27 14:30:46,613 - INFO - Step 11700/100000 | Loss: 0.0201 | LR: 9.70e-05
2025-10-27 14:30:52,669 - INFO - Step 11800/100000 | Loss: 0.0173 | LR: 9.69e-05
2025-10-27 14:30:58,745 - INFO - Step 11900/100000 | Loss: 0.0149 | LR: 9.69e-05
2025-10-27 14:31:04,788 - INFO - Step 12000/100000 | Loss: 0.0178 | LR: 9.68e-05
2025-10-27 14:31:10,839 - INFO - Step 12100/100000 | Loss: 0.0192 | LR: 9.68e-05
2025-10-27 14:31:16,900 - INFO - Step 12200/100000 | Loss: 0.0202 | LR: 9.67e-05
2025-10-27 14:31:22,940 - INFO - Step 12300/100000 | Loss: 0.0173 | LR: 9.67e-05
2025-10-27 14:31:29,018 - INFO - Step 12400/100000 | Loss: 0.0168 | LR: 9.66e-05
2025-10-27 14:31:35,072 - INFO - Step 12500/100000 | Loss: 0.0169 | LR: 9.66e-05
2025-10-27 14:31:41,104 - INFO - Step 12600/100000 | Loss: 0.0188 | LR: 9.65e-05
2025-10-27 14:31:47,205 - INFO - Step 12700/100000 | Loss: 0.0161 | LR: 9.65e-05
2025-10-27 14:31:53,286 - INFO - Step 12800/100000 | Loss: 0.0224 | LR: 9.64e-05
2025-10-27 14:31:59,432 - INFO - Step 12900/100000 | Loss: 0.0195 | LR: 9.64e-05
2025-10-27 14:32:05,551 - INFO - Step 13000/100000 | Loss: 0.0174 | LR: 9.63e-05
2025-10-27 14:32:11,664 - INFO - Step 13100/100000 | Loss: 0.0185 | LR: 9.62e-05
2025-10-27 14:32:17,709 - INFO - Step 13200/100000 | Loss: 0.0195 | LR: 9.62e-05
2025-10-27 14:32:23,739 - INFO - Step 13300/100000 | Loss: 0.0174 | LR: 9.61e-05
2025-10-27 14:32:29,768 - INFO - Step 13400/100000 | Loss: 0.0177 | LR: 9.61e-05
2025-10-27 14:32:35,803 - INFO - Step 13500/100000 | Loss: 0.0186 | LR: 9.60e-05
2025-10-27 14:32:41,816 - INFO - Step 13600/100000 | Loss: 0.0178 | LR: 9.60e-05
2025-10-27 14:32:47,867 - INFO - Step 13700/100000 | Loss: 0.0191 | LR: 9.59e-05
2025-10-27 14:32:53,978 - INFO - Step 13800/100000 | Loss: 0.0168 | LR: 9.58e-05
2025-10-27 14:33:00,030 - INFO - Step 13900/100000 | Loss: 0.0176 | LR: 9.58e-05
2025-10-27 14:33:06,077 - INFO - Step 14000/100000 | Loss: 0.0157 | LR: 9.57e-05
2025-10-27 14:33:12,124 - INFO - Step 14100/100000 | Loss: 0.0184 | LR: 9.57e-05
2025-10-27 14:33:18,190 - INFO - Step 14200/100000 | Loss: 0.0176 | LR: 9.56e-05
2025-10-27 14:33:24,247 - INFO - Step 14300/100000 | Loss: 0.0168 | LR: 9.55e-05
2025-10-27 14:33:30,280 - INFO - Step 14400/100000 | Loss: 0.0167 | LR: 9.55e-05
2025-10-27 14:33:36,328 - INFO - Step 14500/100000 | Loss: 0.0155 | LR: 9.54e-05
2025-10-27 14:33:42,390 - INFO - Step 14600/100000 | Loss: 0.0183 | LR: 9.53e-05
2025-10-27 14:33:48,439 - INFO - Step 14700/100000 | Loss: 0.0171 | LR: 9.53e-05
2025-10-27 14:33:54,508 - INFO - Step 14800/100000 | Loss: 0.0167 | LR: 9.52e-05
2025-10-27 14:34:00,557 - INFO - Step 14900/100000 | Loss: 0.0160 | LR: 9.52e-05
2025-10-27 14:34:06,626 - INFO - Step 15000/100000 | Loss: 0.0175 | LR: 9.51e-05
2025-10-27 14:34:12,688 - INFO - Step 15100/100000 | Loss: 0.0156 | LR: 9.50e-05
2025-10-27 14:34:18,709 - INFO - Step 15200/100000 | Loss: 0.0177 | LR: 9.50e-05
2025-10-27 14:34:24,724 - INFO - Step 15300/100000 | Loss: 0.0179 | LR: 9.49e-05
2025-10-27 14:34:30,731 - INFO - Step 15400/100000 | Loss: 0.0193 | LR: 9.48e-05
2025-10-27 14:34:36,149 - INFO - Step 15500/100000 | Loss: 0.0165 | LR: 9.48e-05
2025-10-27 14:34:41,640 - INFO - Step 15600/100000 | Loss: 0.0154 | LR: 9.47e-05
2025-10-27 14:34:47,167 - INFO - Step 15700/100000 | Loss: 0.0182 | LR: 9.46e-05
2025-10-27 14:34:52,548 - INFO - Step 15800/100000 | Loss: 0.0176 | LR: 9.46e-05
2025-10-27 14:34:57,911 - INFO - Step 15900/100000 | Loss: 0.0150 | LR: 9.45e-05
2025-10-27 14:35:03,257 - INFO - Step 16000/100000 | Loss: 0.0175 | LR: 9.44e-05
2025-10-27 14:35:08,609 - INFO - Step 16100/100000 | Loss: 0.0153 | LR: 9.44e-05
2025-10-27 14:35:14,016 - INFO - Step 16200/100000 | Loss: 0.0186 | LR: 9.43e-05
2025-10-27 14:35:19,460 - INFO - Step 16300/100000 | Loss: 0.0159 | LR: 9.42e-05
2025-10-27 14:35:24,914 - INFO - Step 16400/100000 | Loss: 0.0152 | LR: 9.42e-05
2025-10-27 14:35:30,307 - INFO - Step 16500/100000 | Loss: 0.0163 | LR: 9.41e-05
2025-10-27 14:35:37,246 - INFO - Step 16600/100000 | Loss: 0.0155 | LR: 9.40e-05
2025-10-27 14:35:42,883 - INFO - Step 16700/100000 | Loss: 0.0162 | LR: 9.39e-05
2025-10-27 14:35:48,775 - INFO - Step 16800/100000 | Loss: 0.0187 | LR: 9.39e-05
2025-10-27 14:35:54,726 - INFO - Step 16900/100000 | Loss: 0.0152 | LR: 9.38e-05
2025-10-27 14:36:00,775 - INFO - Step 17000/100000 | Loss: 0.0158 | LR: 9.37e-05
2025-10-27 14:36:06,807 - INFO - Step 17100/100000 | Loss: 0.0179 | LR: 9.37e-05
2025-10-27 14:36:12,829 - INFO - Step 17200/100000 | Loss: 0.0164 | LR: 9.36e-05
2025-10-27 14:36:18,875 - INFO - Step 17300/100000 | Loss: 0.0143 | LR: 9.35e-05
2025-10-27 14:36:24,920 - INFO - Step 17400/100000 | Loss: 0.0139 | LR: 9.34e-05
2025-10-27 14:36:30,941 - INFO - Step 17500/100000 | Loss: 0.0160 | LR: 9.34e-05
2025-10-27 14:36:37,005 - INFO - Step 17600/100000 | Loss: 0.0156 | LR: 9.33e-05
2025-10-27 14:36:43,025 - INFO - Step 17700/100000 | Loss: 0.0142 | LR: 9.32e-05
2025-10-27 14:36:49,060 - INFO - Step 17800/100000 | Loss: 0.0146 | LR: 9.31e-05
2025-10-27 14:36:55,109 - INFO - Step 17900/100000 | Loss: 0.0166 | LR: 9.31e-05
2025-10-27 14:37:01,142 - INFO - Step 18000/100000 | Loss: 0.0160 | LR: 9.30e-05
2025-10-27 14:37:07,168 - INFO - Step 18100/100000 | Loss: 0.0156 | LR: 9.29e-05
2025-10-27 14:37:13,205 - INFO - Step 18200/100000 | Loss: 0.0160 | LR: 9.28e-05
2025-10-27 14:37:19,235 - INFO - Step 18300/100000 | Loss: 0.0154 | LR: 9.28e-05
2025-10-27 14:37:25,262 - INFO - Step 18400/100000 | Loss: 0.0186 | LR: 9.27e-05
2025-10-27 14:37:31,319 - INFO - Step 18500/100000 | Loss: 0.0167 | LR: 9.26e-05
2025-10-27 14:37:37,384 - INFO - Step 18600/100000 | Loss: 0.0170 | LR: 9.25e-05
2025-10-27 14:37:43,414 - INFO - Step 18700/100000 | Loss: 0.0144 | LR: 9.25e-05
2025-10-27 14:37:49,433 - INFO - Step 18800/100000 | Loss: 0.0140 | LR: 9.24e-05
2025-10-27 14:37:55,447 - INFO - Step 18900/100000 | Loss: 0.0167 | LR: 9.23e-05
2025-10-27 14:38:01,504 - INFO - Step 19000/100000 | Loss: 0.0148 | LR: 9.22e-05
2025-10-27 14:38:07,639 - INFO - Step 19100/100000 | Loss: 0.0160 | LR: 9.21e-05
2025-10-27 14:38:13,711 - INFO - Step 19200/100000 | Loss: 0.0152 | LR: 9.21e-05
2025-10-27 14:38:19,752 - INFO - Step 19300/100000 | Loss: 0.0145 | LR: 9.20e-05
2025-10-27 14:38:25,792 - INFO - Step 19400/100000 | Loss: 0.0146 | LR: 9.19e-05
2025-10-27 14:38:31,848 - INFO - Step 19500/100000 | Loss: 0.0137 | LR: 9.18e-05
2025-10-27 14:38:37,881 - INFO - Step 19600/100000 | Loss: 0.0150 | LR: 9.17e-05
2025-10-27 14:38:43,903 - INFO - Step 19700/100000 | Loss: 0.0147 | LR: 9.17e-05
2025-10-27 14:38:49,923 - INFO - Step 19800/100000 | Loss: 0.0165 | LR: 9.16e-05
2025-10-27 14:38:55,935 - INFO - Step 19900/100000 | Loss: 0.0156 | LR: 9.15e-05
2025-10-27 14:39:01,964 - INFO - Step 20000/100000 | Loss: 0.0155 | LR: 9.14e-05
2025-10-27 14:39:01,965 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_20000
2025-10-27 14:39:04,337 - INFO - New best loss: 0.0069, saving to outputs/train/g1_diffusion/checkpoints/best
2025-10-27 14:39:15,088 - INFO - Step 20100/100000 | Loss: 0.0172 | LR: 9.13e-05
2025-10-27 14:39:21,123 - INFO - Step 20200/100000 | Loss: 0.0153 | LR: 9.12e-05
2025-10-27 14:39:27,174 - INFO - Step 20300/100000 | Loss: 0.0155 | LR: 9.12e-05
2025-10-27 14:39:33,214 - INFO - Step 20400/100000 | Loss: 0.0150 | LR: 9.11e-05
2025-10-27 14:39:39,271 - INFO - Step 20500/100000 | Loss: 0.0160 | LR: 9.10e-05
2025-10-27 14:39:45,291 - INFO - Step 20600/100000 | Loss: 0.0155 | LR: 9.09e-05
2025-10-27 14:39:51,327 - INFO - Step 20700/100000 | Loss: 0.0138 | LR: 9.08e-05
2025-10-27 14:39:57,429 - INFO - Step 20800/100000 | Loss: 0.0148 | LR: 9.07e-05
2025-10-27 14:40:02,869 - INFO - Step 20900/100000 | Loss: 0.0152 | LR: 9.06e-05
2025-10-27 14:40:08,260 - INFO - Step 21000/100000 | Loss: 0.0151 | LR: 9.06e-05
2025-10-27 14:40:13,622 - INFO - Step 21100/100000 | Loss: 0.0138 | LR: 9.05e-05
2025-10-27 14:40:18,936 - INFO - Step 21200/100000 | Loss: 0.0140 | LR: 9.04e-05
2025-10-27 14:40:24,275 - INFO - Step 21300/100000 | Loss: 0.0131 | LR: 9.03e-05
2025-10-27 14:40:29,661 - INFO - Step 21400/100000 | Loss: 0.0163 | LR: 9.02e-05
2025-10-27 14:40:35,016 - INFO - Step 21500/100000 | Loss: 0.0137 | LR: 9.01e-05
2025-10-27 14:40:40,371 - INFO - Step 21600/100000 | Loss: 0.0137 | LR: 9.00e-05
2025-10-27 14:40:45,691 - INFO - Step 21700/100000 | Loss: 0.0156 | LR: 8.99e-05
2025-10-27 14:40:51,082 - INFO - Step 21800/100000 | Loss: 0.0139 | LR: 8.99e-05
2025-10-27 14:40:56,462 - INFO - Step 21900/100000 | Loss: 0.0148 | LR: 8.98e-05
2025-10-27 14:41:01,831 - INFO - Step 22000/100000 | Loss: 0.0116 | LR: 8.97e-05
2025-10-27 14:41:07,221 - INFO - Step 22100/100000 | Loss: 0.0149 | LR: 8.96e-05
2025-10-27 14:41:12,501 - INFO - Step 22200/100000 | Loss: 0.0138 | LR: 8.95e-05
2025-10-27 14:41:17,847 - INFO - Step 22300/100000 | Loss: 0.0142 | LR: 8.94e-05
2025-10-27 14:41:23,158 - INFO - Step 22400/100000 | Loss: 0.0151 | LR: 8.93e-05
2025-10-27 14:41:28,481 - INFO - Step 22500/100000 | Loss: 0.0159 | LR: 8.92e-05
2025-10-27 14:41:33,822 - INFO - Step 22600/100000 | Loss: 0.0147 | LR: 8.91e-05
2025-10-27 14:41:39,199 - INFO - Step 22700/100000 | Loss: 0.0147 | LR: 8.90e-05
2025-10-27 14:41:44,591 - INFO - Step 22800/100000 | Loss: 0.0133 | LR: 8.89e-05
2025-10-27 14:41:49,905 - INFO - Step 22900/100000 | Loss: 0.0149 | LR: 8.88e-05
2025-10-27 14:41:55,225 - INFO - Step 23000/100000 | Loss: 0.0135 | LR: 8.88e-05
2025-10-27 14:42:00,555 - INFO - Step 23100/100000 | Loss: 0.0154 | LR: 8.87e-05
2025-10-27 14:42:05,913 - INFO - Step 23200/100000 | Loss: 0.0167 | LR: 8.86e-05
2025-10-27 14:42:11,246 - INFO - Step 23300/100000 | Loss: 0.0161 | LR: 8.85e-05
2025-10-27 14:42:15,027 - INFO - Completed epoch 2
2025-10-27 14:42:16,923 - INFO - Step 23400/100000 | Loss: 0.0143 | LR: 8.84e-05
2025-10-27 14:42:22,273 - INFO - Step 23500/100000 | Loss: 0.0139 | LR: 8.83e-05
2025-10-27 14:42:27,631 - INFO - Step 23600/100000 | Loss: 0.0139 | LR: 8.82e-05
2025-10-27 14:42:32,978 - INFO - Step 23700/100000 | Loss: 0.0146 | LR: 8.81e-05
2025-10-27 14:42:38,303 - INFO - Step 23800/100000 | Loss: 0.0145 | LR: 8.80e-05
2025-10-27 14:42:43,595 - INFO - Step 23900/100000 | Loss: 0.0138 | LR: 8.79e-05
2025-10-27 14:42:48,895 - INFO - Step 24000/100000 | Loss: 0.0119 | LR: 8.78e-05
2025-10-27 14:42:54,170 - INFO - Step 24100/100000 | Loss: 0.0134 | LR: 8.77e-05
2025-10-27 14:42:59,483 - INFO - Step 24200/100000 | Loss: 0.0131 | LR: 8.76e-05
2025-10-27 14:43:04,841 - INFO - Step 24300/100000 | Loss: 0.0138 | LR: 8.75e-05
2025-10-27 14:43:10,175 - INFO - Step 24400/100000 | Loss: 0.0112 | LR: 8.74e-05
2025-10-27 14:43:15,559 - INFO - Step 24500/100000 | Loss: 0.0131 | LR: 8.73e-05
2025-10-27 14:43:20,918 - INFO - Step 24600/100000 | Loss: 0.0127 | LR: 8.72e-05
2025-10-27 14:43:26,286 - INFO - Step 24700/100000 | Loss: 0.0126 | LR: 8.71e-05
2025-10-27 14:43:31,657 - INFO - Step 24800/100000 | Loss: 0.0135 | LR: 8.70e-05
2025-10-27 14:43:37,057 - INFO - Step 24900/100000 | Loss: 0.0159 | LR: 8.69e-05
2025-10-27 14:43:42,314 - INFO - Step 25000/100000 | Loss: 0.0130 | LR: 8.68e-05
2025-10-27 14:43:47,652 - INFO - Step 25100/100000 | Loss: 0.0146 | LR: 8.67e-05
2025-10-27 14:43:53,001 - INFO - Step 25200/100000 | Loss: 0.0131 | LR: 8.66e-05
2025-10-27 14:43:58,346 - INFO - Step 25300/100000 | Loss: 0.0131 | LR: 8.65e-05
2025-10-27 14:44:03,686 - INFO - Step 25400/100000 | Loss: 0.0125 | LR: 8.64e-05
2025-10-27 14:44:09,064 - INFO - Step 25500/100000 | Loss: 0.0121 | LR: 8.63e-05
2025-10-27 14:44:14,370 - INFO - Step 25600/100000 | Loss: 0.0138 | LR: 8.62e-05
2025-10-27 14:44:19,671 - INFO - Step 25700/100000 | Loss: 0.0124 | LR: 8.61e-05
2025-10-27 14:44:25,001 - INFO - Step 25800/100000 | Loss: 0.0130 | LR: 8.60e-05
2025-10-27 14:44:30,380 - INFO - Step 25900/100000 | Loss: 0.0148 | LR: 8.59e-05
2025-10-27 14:44:35,747 - INFO - Step 26000/100000 | Loss: 0.0136 | LR: 8.58e-05
2025-10-27 14:44:41,118 - INFO - Step 26100/100000 | Loss: 0.0127 | LR: 8.57e-05
2025-10-27 14:44:46,503 - INFO - Step 26200/100000 | Loss: 0.0132 | LR: 8.56e-05
2025-10-27 14:44:51,883 - INFO - Step 26300/100000 | Loss: 0.0129 | LR: 8.55e-05
2025-10-27 14:44:57,291 - INFO - Step 26400/100000 | Loss: 0.0126 | LR: 8.54e-05
2025-10-27 14:45:02,635 - INFO - Step 26500/100000 | Loss: 0.0150 | LR: 8.53e-05
2025-10-27 14:45:07,926 - INFO - Step 26600/100000 | Loss: 0.0165 | LR: 8.52e-05
2025-10-27 14:45:13,228 - INFO - Step 26700/100000 | Loss: 0.0143 | LR: 8.51e-05
2025-10-27 14:45:18,534 - INFO - Step 26800/100000 | Loss: 0.0142 | LR: 8.50e-05
2025-10-27 14:45:23,823 - INFO - Step 26900/100000 | Loss: 0.0122 | LR: 8.49e-05
2025-10-27 14:45:29,109 - INFO - Step 27000/100000 | Loss: 0.0124 | LR: 8.48e-05
2025-10-27 14:45:34,421 - INFO - Step 27100/100000 | Loss: 0.0140 | LR: 8.47e-05
2025-10-27 14:45:39,789 - INFO - Step 27200/100000 | Loss: 0.0111 | LR: 8.45e-05
2025-10-27 14:45:45,141 - INFO - Step 27300/100000 | Loss: 0.0127 | LR: 8.44e-05
2025-10-27 14:45:50,448 - INFO - Step 27400/100000 | Loss: 0.0127 | LR: 8.43e-05
2025-10-27 14:45:55,753 - INFO - Step 27500/100000 | Loss: 0.0124 | LR: 8.42e-05
2025-10-27 14:46:01,066 - INFO - Step 27600/100000 | Loss: 0.0115 | LR: 8.41e-05
2025-10-27 14:46:06,430 - INFO - Step 27700/100000 | Loss: 0.0127 | LR: 8.40e-05
2025-10-27 14:46:11,785 - INFO - Step 27800/100000 | Loss: 0.0110 | LR: 8.39e-05
2025-10-27 14:46:17,144 - INFO - Step 27900/100000 | Loss: 0.0127 | LR: 8.38e-05
2025-10-27 14:46:22,432 - INFO - Step 28000/100000 | Loss: 0.0127 | LR: 8.37e-05
2025-10-27 14:46:27,745 - INFO - Step 28100/100000 | Loss: 0.0151 | LR: 8.36e-05
2025-10-27 14:46:33,099 - INFO - Step 28200/100000 | Loss: 0.0111 | LR: 8.35e-05
2025-10-27 14:46:38,400 - INFO - Step 28300/100000 | Loss: 0.0140 | LR: 8.34e-05
2025-10-27 14:46:43,779 - INFO - Step 28400/100000 | Loss: 0.0129 | LR: 8.32e-05
2025-10-27 14:46:49,101 - INFO - Step 28500/100000 | Loss: 0.0129 | LR: 8.31e-05
2025-10-27 14:46:54,381 - INFO - Step 28600/100000 | Loss: 0.0124 | LR: 8.30e-05
2025-10-27 14:46:59,670 - INFO - Step 28700/100000 | Loss: 0.0137 | LR: 8.29e-05
2025-10-27 14:47:04,982 - INFO - Step 28800/100000 | Loss: 0.0131 | LR: 8.28e-05
2025-10-27 14:47:10,338 - INFO - Step 28900/100000 | Loss: 0.0125 | LR: 8.27e-05
2025-10-27 14:47:15,685 - INFO - Step 29000/100000 | Loss: 0.0139 | LR: 8.26e-05
2025-10-27 14:47:21,041 - INFO - Step 29100/100000 | Loss: 0.0118 | LR: 8.25e-05
2025-10-27 14:47:26,388 - INFO - Step 29200/100000 | Loss: 0.0161 | LR: 8.24e-05
2025-10-27 14:47:31,766 - INFO - Step 29300/100000 | Loss: 0.0133 | LR: 8.22e-05
2025-10-27 14:47:37,149 - INFO - Step 29400/100000 | Loss: 0.0131 | LR: 8.21e-05
2025-10-27 14:47:42,475 - INFO - Step 29500/100000 | Loss: 0.0118 | LR: 8.20e-05
2025-10-27 14:47:47,824 - INFO - Step 29600/100000 | Loss: 0.0115 | LR: 8.19e-05
2025-10-27 14:47:53,174 - INFO - Step 29700/100000 | Loss: 0.0133 | LR: 8.18e-05
2025-10-27 14:47:58,545 - INFO - Step 29800/100000 | Loss: 0.0121 | LR: 8.17e-05
2025-10-27 14:48:03,844 - INFO - Step 29900/100000 | Loss: 0.0132 | LR: 8.16e-05
2025-10-27 14:48:09,149 - INFO - Step 30000/100000 | Loss: 0.0143 | LR: 8.14e-05
2025-10-27 14:48:09,149 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_30000
2025-10-27 14:48:11,536 - INFO - New best loss: 0.0058, saving to outputs/train/g1_diffusion/checkpoints/best
2025-10-27 14:48:18,041 - INFO - Step 30100/100000 | Loss: 0.0129 | LR: 8.13e-05
2025-10-27 14:48:23,331 - INFO - Step 30200/100000 | Loss: 0.0133 | LR: 8.12e-05
2025-10-27 14:48:28,613 - INFO - Step 30300/100000 | Loss: 0.0104 | LR: 8.11e-05
2025-10-27 14:48:33,910 - INFO - Step 30400/100000 | Loss: 0.0142 | LR: 8.10e-05
2025-10-27 14:48:39,226 - INFO - Step 30500/100000 | Loss: 0.0132 | LR: 8.09e-05
2025-10-27 14:48:44,521 - INFO - Step 30600/100000 | Loss: 0.0138 | LR: 8.08e-05
2025-10-27 14:48:49,817 - INFO - Step 30700/100000 | Loss: 0.0109 | LR: 8.06e-05
2025-10-27 14:48:55,152 - INFO - Step 30800/100000 | Loss: 0.0131 | LR: 8.05e-05
2025-10-27 14:49:00,474 - INFO - Step 30900/100000 | Loss: 0.0123 | LR: 8.04e-05
2025-10-27 14:49:05,776 - INFO - Step 31000/100000 | Loss: 0.0124 | LR: 8.03e-05
2025-10-27 14:49:11,117 - INFO - Step 31100/100000 | Loss: 0.0137 | LR: 8.02e-05
2025-10-27 14:49:16,464 - INFO - Step 31200/100000 | Loss: 0.0122 | LR: 8.01e-05
2025-10-27 14:49:21,765 - INFO - Step 31300/100000 | Loss: 0.0125 | LR: 7.99e-05
2025-10-27 14:49:27,045 - INFO - Step 31400/100000 | Loss: 0.0129 | LR: 7.98e-05
2025-10-27 14:49:32,403 - INFO - Step 31500/100000 | Loss: 0.0125 | LR: 7.97e-05
2025-10-27 14:49:37,749 - INFO - Step 31600/100000 | Loss: 0.0118 | LR: 7.96e-05
2025-10-27 14:49:43,089 - INFO - Step 31700/100000 | Loss: 0.0117 | LR: 7.95e-05
2025-10-27 14:49:48,386 - INFO - Step 31800/100000 | Loss: 0.0126 | LR: 7.93e-05
2025-10-27 14:49:53,653 - INFO - Step 31900/100000 | Loss: 0.0121 | LR: 7.92e-05
2025-10-27 14:49:58,944 - INFO - Step 32000/100000 | Loss: 0.0120 | LR: 7.91e-05
2025-10-27 14:50:04,288 - INFO - Step 32100/100000 | Loss: 0.0122 | LR: 7.90e-05
2025-10-27 14:50:09,604 - INFO - Step 32200/100000 | Loss: 0.0117 | LR: 7.89e-05
2025-10-27 14:50:14,918 - INFO - Step 32300/100000 | Loss: 0.0125 | LR: 7.88e-05
2025-10-27 14:50:20,267 - INFO - Step 32400/100000 | Loss: 0.0134 | LR: 7.86e-05
2025-10-27 14:50:25,589 - INFO - Step 32500/100000 | Loss: 0.0121 | LR: 7.85e-05
2025-10-27 14:50:30,966 - INFO - Step 32600/100000 | Loss: 0.0127 | LR: 7.84e-05
2025-10-27 14:50:36,363 - INFO - Step 32700/100000 | Loss: 0.0149 | LR: 7.83e-05
2025-10-27 14:50:41,665 - INFO - Step 32800/100000 | Loss: 0.0125 | LR: 7.81e-05
2025-10-27 14:50:46,990 - INFO - Step 32900/100000 | Loss: 0.0118 | LR: 7.80e-05
2025-10-27 14:50:52,352 - INFO - Step 33000/100000 | Loss: 0.0126 | LR: 7.79e-05
2025-10-27 14:50:57,889 - INFO - Step 33100/100000 | Loss: 0.0132 | LR: 7.78e-05
2025-10-27 14:51:03,296 - INFO - Step 33200/100000 | Loss: 0.0104 | LR: 7.77e-05
2025-10-27 14:51:08,674 - INFO - Step 33300/100000 | Loss: 0.0130 | LR: 7.75e-05
2025-10-27 14:51:14,130 - INFO - Step 33400/100000 | Loss: 0.0126 | LR: 7.74e-05
2025-10-27 14:51:19,606 - INFO - Step 33500/100000 | Loss: 0.0107 | LR: 7.73e-05
2025-10-27 14:51:25,021 - INFO - Step 33600/100000 | Loss: 0.0122 | LR: 7.72e-05
2025-10-27 14:51:30,454 - INFO - Step 33700/100000 | Loss: 0.0149 | LR: 7.70e-05
2025-10-27 14:51:35,957 - INFO - Step 33800/100000 | Loss: 0.0125 | LR: 7.69e-05
2025-10-27 14:51:41,460 - INFO - Step 33900/100000 | Loss: 0.0108 | LR: 7.68e-05
2025-10-27 14:51:46,920 - INFO - Step 34000/100000 | Loss: 0.0124 | LR: 7.67e-05
2025-10-27 14:51:52,409 - INFO - Step 34100/100000 | Loss: 0.0114 | LR: 7.66e-05
2025-10-27 14:51:57,822 - INFO - Step 34200/100000 | Loss: 0.0114 | LR: 7.64e-05
2025-10-27 14:52:03,247 - INFO - Step 34300/100000 | Loss: 0.0100 | LR: 7.63e-05
2025-10-27 14:52:08,647 - INFO - Step 34400/100000 | Loss: 0.0101 | LR: 7.62e-05
2025-10-27 14:52:14,024 - INFO - Step 34500/100000 | Loss: 0.0100 | LR: 7.61e-05
2025-10-27 14:52:19,406 - INFO - Step 34600/100000 | Loss: 0.0107 | LR: 7.59e-05
2025-10-27 14:52:24,885 - INFO - Step 34700/100000 | Loss: 0.0132 | LR: 7.58e-05
2025-10-27 14:52:32,106 - INFO - Step 34800/100000 | Loss: 0.0121 | LR: 7.57e-05
2025-10-27 14:52:37,673 - INFO - Step 34900/100000 | Loss: 0.0114 | LR: 7.56e-05
2025-10-27 14:52:43,774 - INFO - Step 35000/100000 | Loss: 0.0125 | LR: 7.54e-05
2025-10-27 14:52:46,974 - INFO - Completed epoch 3
2025-10-27 14:52:50,133 - INFO - Step 35100/100000 | Loss: 0.0118 | LR: 7.53e-05
2025-10-27 14:52:56,213 - INFO - Step 35200/100000 | Loss: 0.0119 | LR: 7.52e-05
2025-10-27 14:53:02,329 - INFO - Step 35300/100000 | Loss: 0.0132 | LR: 7.50e-05
2025-10-27 14:53:08,417 - INFO - Step 35400/100000 | Loss: 0.0128 | LR: 7.49e-05
2025-10-27 14:53:14,450 - INFO - Step 35500/100000 | Loss: 0.0112 | LR: 7.48e-05
2025-10-27 14:53:20,531 - INFO - Step 35600/100000 | Loss: 0.0118 | LR: 7.47e-05
2025-10-27 14:53:26,597 - INFO - Step 35700/100000 | Loss: 0.0112 | LR: 7.45e-05
2025-10-27 14:53:32,698 - INFO - Step 35800/100000 | Loss: 0.0102 | LR: 7.44e-05
2025-10-27 14:53:38,740 - INFO - Step 35900/100000 | Loss: 0.0101 | LR: 7.43e-05
2025-10-27 14:53:44,795 - INFO - Step 36000/100000 | Loss: 0.0130 | LR: 7.42e-05
2025-10-27 14:53:50,851 - INFO - Step 36100/100000 | Loss: 0.0104 | LR: 7.40e-05
2025-10-27 14:53:56,920 - INFO - Step 36200/100000 | Loss: 0.0110 | LR: 7.39e-05
2025-10-27 14:54:02,970 - INFO - Step 36300/100000 | Loss: 0.0113 | LR: 7.38e-05
2025-10-27 14:54:08,505 - INFO - Step 36400/100000 | Loss: 0.0114 | LR: 7.36e-05
2025-10-27 14:54:13,970 - INFO - Step 36500/100000 | Loss: 0.0127 | LR: 7.35e-05
2025-10-27 14:54:19,489 - INFO - Step 36600/100000 | Loss: 0.0119 | LR: 7.34e-05
2025-10-27 14:54:24,944 - INFO - Step 36700/100000 | Loss: 0.0123 | LR: 7.33e-05
2025-10-27 14:54:30,369 - INFO - Step 36800/100000 | Loss: 0.0102 | LR: 7.31e-05
2025-10-27 14:54:35,827 - INFO - Step 36900/100000 | Loss: 0.0121 | LR: 7.30e-05
2025-10-27 14:54:41,261 - INFO - Step 37000/100000 | Loss: 0.0106 | LR: 7.29e-05
2025-10-27 14:54:46,661 - INFO - Step 37100/100000 | Loss: 0.0126 | LR: 7.27e-05
2025-10-27 14:54:52,006 - INFO - Step 37200/100000 | Loss: 0.0125 | LR: 7.26e-05
2025-10-27 14:54:57,403 - INFO - Step 37300/100000 | Loss: 0.0123 | LR: 7.25e-05
2025-10-27 14:55:02,847 - INFO - Step 37400/100000 | Loss: 0.0108 | LR: 7.23e-05
2025-10-27 14:55:08,436 - INFO - Step 37500/100000 | Loss: 0.0128 | LR: 7.22e-05
2025-10-27 14:55:15,661 - INFO - Step 37600/100000 | Loss: 0.0101 | LR: 7.21e-05
2025-10-27 14:55:21,520 - INFO - Step 37700/100000 | Loss: 0.0114 | LR: 7.20e-05
2025-10-27 14:55:27,527 - INFO - Step 37800/100000 | Loss: 0.0120 | LR: 7.18e-05
2025-10-27 14:55:33,560 - INFO - Step 37900/100000 | Loss: 0.0105 | LR: 7.17e-05
2025-10-27 14:55:39,608 - INFO - Step 38000/100000 | Loss: 0.0110 | LR: 7.16e-05
2025-10-27 14:55:45,584 - INFO - Step 38100/100000 | Loss: 0.0114 | LR: 7.14e-05
2025-10-27 14:55:51,558 - INFO - Step 38200/100000 | Loss: 0.0097 | LR: 7.13e-05
2025-10-27 14:55:57,563 - INFO - Step 38300/100000 | Loss: 0.0118 | LR: 7.12e-05
2025-10-27 14:56:03,507 - INFO - Step 38400/100000 | Loss: 0.0100 | LR: 7.10e-05
2025-10-27 14:56:09,476 - INFO - Step 38500/100000 | Loss: 0.0119 | LR: 7.09e-05
2025-10-27 14:56:15,456 - INFO - Step 38600/100000 | Loss: 0.0108 | LR: 7.08e-05
2025-10-27 14:56:21,492 - INFO - Step 38700/100000 | Loss: 0.0099 | LR: 7.06e-05
2025-10-27 14:56:27,531 - INFO - Step 38800/100000 | Loss: 0.0109 | LR: 7.05e-05
2025-10-27 14:56:33,540 - INFO - Step 38900/100000 | Loss: 0.0115 | LR: 7.04e-05
2025-10-27 14:56:39,502 - INFO - Step 39000/100000 | Loss: 0.0105 | LR: 7.02e-05
2025-10-27 14:56:45,486 - INFO - Step 39100/100000 | Loss: 0.0126 | LR: 7.01e-05
2025-10-27 14:56:51,461 - INFO - Step 39200/100000 | Loss: 0.0113 | LR: 7.00e-05
2025-10-27 14:56:57,478 - INFO - Step 39300/100000 | Loss: 0.0107 | LR: 6.98e-05
2025-10-27 14:57:03,488 - INFO - Step 39400/100000 | Loss: 0.0109 | LR: 6.97e-05
2025-10-27 14:57:09,479 - INFO - Step 39500/100000 | Loss: 0.0112 | LR: 6.96e-05
2025-10-27 14:57:15,496 - INFO - Step 39600/100000 | Loss: 0.0105 | LR: 6.94e-05
2025-10-27 14:57:21,456 - INFO - Step 39700/100000 | Loss: 0.0106 | LR: 6.93e-05
2025-10-27 14:57:27,457 - INFO - Step 39800/100000 | Loss: 0.0117 | LR: 6.92e-05
2025-10-27 14:57:33,430 - INFO - Step 39900/100000 | Loss: 0.0099 | LR: 6.90e-05
2025-10-27 14:57:39,386 - INFO - Step 40000/100000 | Loss: 0.0123 | LR: 6.89e-05
2025-10-27 14:57:39,387 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_40000
2025-10-27 14:57:47,775 - INFO - Step 40100/100000 | Loss: 0.0115 | LR: 6.88e-05
2025-10-27 14:57:53,756 - INFO - Step 40200/100000 | Loss: 0.0098 | LR: 6.86e-05
2025-10-27 14:57:59,719 - INFO - Step 40300/100000 | Loss: 0.0118 | LR: 6.85e-05
2025-10-27 14:58:05,722 - INFO - Step 40400/100000 | Loss: 0.0107 | LR: 6.84e-05
2025-10-27 14:58:11,713 - INFO - Step 40500/100000 | Loss: 0.0106 | LR: 6.82e-05
2025-10-27 14:58:17,706 - INFO - Step 40600/100000 | Loss: 0.0117 | LR: 6.81e-05
2025-10-27 14:58:23,704 - INFO - Step 40700/100000 | Loss: 0.0092 | LR: 6.80e-05
2025-10-27 14:58:29,703 - INFO - Step 40800/100000 | Loss: 0.0102 | LR: 6.78e-05
2025-10-27 14:58:35,693 - INFO - Step 40900/100000 | Loss: 0.0099 | LR: 6.77e-05
2025-10-27 14:58:41,646 - INFO - Step 41000/100000 | Loss: 0.0129 | LR: 6.76e-05
2025-10-27 14:58:47,594 - INFO - Step 41100/100000 | Loss: 0.0119 | LR: 6.74e-05
2025-10-27 14:58:53,592 - INFO - Step 41200/100000 | Loss: 0.0099 | LR: 6.73e-05
2025-10-27 14:58:59,580 - INFO - Step 41300/100000 | Loss: 0.0107 | LR: 6.71e-05
2025-10-27 14:59:05,541 - INFO - Step 41400/100000 | Loss: 0.0123 | LR: 6.70e-05
2025-10-27 14:59:11,564 - INFO - Step 41500/100000 | Loss: 0.0122 | LR: 6.69e-05
2025-10-27 14:59:17,556 - INFO - Step 41600/100000 | Loss: 0.0101 | LR: 6.67e-05
2025-10-27 14:59:23,536 - INFO - Step 41700/100000 | Loss: 0.0111 | LR: 6.66e-05
2025-10-27 14:59:29,568 - INFO - Step 41800/100000 | Loss: 0.0105 | LR: 6.65e-05
2025-10-27 14:59:35,562 - INFO - Step 41900/100000 | Loss: 0.0103 | LR: 6.63e-05
2025-10-27 14:59:41,570 - INFO - Step 42000/100000 | Loss: 0.0107 | LR: 6.62e-05
2025-10-27 14:59:47,571 - INFO - Step 42100/100000 | Loss: 0.0100 | LR: 6.61e-05
2025-10-27 14:59:53,577 - INFO - Step 42200/100000 | Loss: 0.0097 | LR: 6.59e-05
2025-10-27 14:59:59,579 - INFO - Step 42300/100000 | Loss: 0.0109 | LR: 6.58e-05
2025-10-27 15:00:05,558 - INFO - Step 42400/100000 | Loss: 0.0112 | LR: 6.56e-05
2025-10-27 15:00:11,581 - INFO - Step 42500/100000 | Loss: 0.0104 | LR: 6.55e-05
2025-10-27 15:00:17,584 - INFO - Step 42600/100000 | Loss: 0.0112 | LR: 6.54e-05
2025-10-27 15:00:23,566 - INFO - Step 42700/100000 | Loss: 0.0109 | LR: 6.52e-05
2025-10-27 15:00:29,555 - INFO - Step 42800/100000 | Loss: 0.0111 | LR: 6.51e-05
2025-10-27 15:00:35,545 - INFO - Step 42900/100000 | Loss: 0.0105 | LR: 6.50e-05
2025-10-27 15:00:41,579 - INFO - Step 43000/100000 | Loss: 0.0111 | LR: 6.48e-05
2025-10-27 15:00:47,561 - INFO - Step 43100/100000 | Loss: 0.0094 | LR: 6.47e-05
2025-10-27 15:00:53,565 - INFO - Step 43200/100000 | Loss: 0.0108 | LR: 6.45e-05
2025-10-27 15:00:59,547 - INFO - Step 43300/100000 | Loss: 0.0111 | LR: 6.44e-05
2025-10-27 15:01:05,514 - INFO - Step 43400/100000 | Loss: 0.0095 | LR: 6.43e-05
2025-10-27 15:01:11,486 - INFO - Step 43500/100000 | Loss: 0.0107 | LR: 6.41e-05
2025-10-27 15:01:17,491 - INFO - Step 43600/100000 | Loss: 0.0101 | LR: 6.40e-05
2025-10-27 15:01:23,511 - INFO - Step 43700/100000 | Loss: 0.0115 | LR: 6.38e-05
2025-10-27 15:01:29,512 - INFO - Step 43800/100000 | Loss: 0.0112 | LR: 6.37e-05
2025-10-27 15:01:35,533 - INFO - Step 43900/100000 | Loss: 0.0101 | LR: 6.36e-05
2025-10-27 15:01:41,514 - INFO - Step 44000/100000 | Loss: 0.0105 | LR: 6.34e-05
2025-10-27 15:01:47,490 - INFO - Step 44100/100000 | Loss: 0.0102 | LR: 6.33e-05
2025-10-27 15:01:53,506 - INFO - Step 44200/100000 | Loss: 0.0110 | LR: 6.32e-05
2025-10-27 15:01:59,508 - INFO - Step 44300/100000 | Loss: 0.0110 | LR: 6.30e-05
2025-10-27 15:02:05,494 - INFO - Step 44400/100000 | Loss: 0.0098 | LR: 6.29e-05
2025-10-27 15:02:11,514 - INFO - Step 44500/100000 | Loss: 0.0104 | LR: 6.27e-05
2025-10-27 15:02:17,454 - INFO - Step 44600/100000 | Loss: 0.0104 | LR: 6.26e-05
2025-10-27 15:02:23,458 - INFO - Step 44700/100000 | Loss: 0.0114 | LR: 6.25e-05
2025-10-27 15:02:29,444 - INFO - Step 44800/100000 | Loss: 0.0113 | LR: 6.23e-05
2025-10-27 15:02:35,442 - INFO - Step 44900/100000 | Loss: 0.0102 | LR: 6.22e-05
2025-10-27 15:02:41,355 - INFO - Step 45000/100000 | Loss: 0.0091 | LR: 6.20e-05
2025-10-27 15:02:46,723 - INFO - Step 45100/100000 | Loss: 0.0103 | LR: 6.19e-05
2025-10-27 15:02:52,061 - INFO - Step 45200/100000 | Loss: 0.0115 | LR: 6.18e-05
2025-10-27 15:02:57,353 - INFO - Step 45300/100000 | Loss: 0.0104 | LR: 6.16e-05
2025-10-27 15:03:02,676 - INFO - Step 45400/100000 | Loss: 0.0093 | LR: 6.15e-05
2025-10-27 15:03:07,958 - INFO - Step 45500/100000 | Loss: 0.0105 | LR: 6.13e-05
2025-10-27 15:03:13,237 - INFO - Step 45600/100000 | Loss: 0.0079 | LR: 6.12e-05
2025-10-27 15:03:18,534 - INFO - Step 45700/100000 | Loss: 0.0095 | LR: 6.11e-05
2025-10-27 15:03:23,809 - INFO - Step 45800/100000 | Loss: 0.0117 | LR: 6.09e-05
2025-10-27 15:03:29,071 - INFO - Step 45900/100000 | Loss: 0.0109 | LR: 6.08e-05
2025-10-27 15:03:34,359 - INFO - Step 46000/100000 | Loss: 0.0098 | LR: 6.06e-05
2025-10-27 15:03:39,681 - INFO - Step 46100/100000 | Loss: 0.0102 | LR: 6.05e-05
2025-10-27 15:03:44,994 - INFO - Step 46200/100000 | Loss: 0.0108 | LR: 6.04e-05
2025-10-27 15:03:50,282 - INFO - Step 46300/100000 | Loss: 0.0097 | LR: 6.02e-05
2025-10-27 15:03:55,518 - INFO - Step 46400/100000 | Loss: 0.0108 | LR: 6.01e-05
2025-10-27 15:04:00,801 - INFO - Step 46500/100000 | Loss: 0.0099 | LR: 5.99e-05
2025-10-27 15:04:06,106 - INFO - Step 46600/100000 | Loss: 0.0103 | LR: 5.98e-05
2025-10-27 15:04:11,403 - INFO - Step 46700/100000 | Loss: 0.0096 | LR: 5.97e-05
2025-10-27 15:04:13,338 - INFO - Completed epoch 4
2025-10-27 15:04:16,992 - INFO - Step 46800/100000 | Loss: 0.0103 | LR: 5.95e-05
2025-10-27 15:04:22,276 - INFO - Step 46900/100000 | Loss: 0.0107 | LR: 5.94e-05
2025-10-27 15:04:27,556 - INFO - Step 47000/100000 | Loss: 0.0106 | LR: 5.92e-05
2025-10-27 15:04:32,865 - INFO - Step 47100/100000 | Loss: 0.0097 | LR: 5.91e-05
2025-10-27 15:04:38,175 - INFO - Step 47200/100000 | Loss: 0.0099 | LR: 5.90e-05
2025-10-27 15:04:43,464 - INFO - Step 47300/100000 | Loss: 0.0101 | LR: 5.88e-05
2025-10-27 15:04:48,786 - INFO - Step 47400/100000 | Loss: 0.0099 | LR: 5.87e-05
2025-10-27 15:04:54,103 - INFO - Step 47500/100000 | Loss: 0.0095 | LR: 5.85e-05
2025-10-27 15:04:59,393 - INFO - Step 47600/100000 | Loss: 0.0107 | LR: 5.84e-05
2025-10-27 15:05:04,720 - INFO - Step 47700/100000 | Loss: 0.0098 | LR: 5.82e-05
2025-10-27 15:05:09,996 - INFO - Step 47800/100000 | Loss: 0.0091 | LR: 5.81e-05
2025-10-27 15:05:15,270 - INFO - Step 47900/100000 | Loss: 0.0105 | LR: 5.80e-05
2025-10-27 15:05:20,544 - INFO - Step 48000/100000 | Loss: 0.0103 | LR: 5.78e-05
2025-10-27 15:05:25,863 - INFO - Step 48100/100000 | Loss: 0.0111 | LR: 5.77e-05
2025-10-27 15:05:31,187 - INFO - Step 48200/100000 | Loss: 0.0116 | LR: 5.75e-05
2025-10-27 15:05:36,502 - INFO - Step 48300/100000 | Loss: 0.0100 | LR: 5.74e-05
2025-10-27 15:05:41,854 - INFO - Step 48400/100000 | Loss: 0.0080 | LR: 5.73e-05
2025-10-27 15:05:47,210 - INFO - Step 48500/100000 | Loss: 0.0091 | LR: 5.71e-05
2025-10-27 15:05:52,553 - INFO - Step 48600/100000 | Loss: 0.0109 | LR: 5.70e-05
2025-10-27 15:05:57,949 - INFO - Step 48700/100000 | Loss: 0.0088 | LR: 5.68e-05
2025-10-27 15:06:03,320 - INFO - Step 48800/100000 | Loss: 0.0106 | LR: 5.67e-05
2025-10-27 15:06:08,626 - INFO - Step 48900/100000 | Loss: 0.0113 | LR: 5.66e-05
2025-10-27 15:06:13,926 - INFO - Step 49000/100000 | Loss: 0.0096 | LR: 5.64e-05
2025-10-27 15:06:19,318 - INFO - Step 49100/100000 | Loss: 0.0114 | LR: 5.63e-05
2025-10-27 15:06:24,662 - INFO - Step 49200/100000 | Loss: 0.0116 | LR: 5.61e-05
2025-10-27 15:06:30,026 - INFO - Step 49300/100000 | Loss: 0.0096 | LR: 5.60e-05
2025-10-27 15:06:35,372 - INFO - Step 49400/100000 | Loss: 0.0097 | LR: 5.58e-05
2025-10-27 15:06:40,653 - INFO - Step 49500/100000 | Loss: 0.0085 | LR: 5.57e-05
2025-10-27 15:06:45,955 - INFO - Step 49600/100000 | Loss: 0.0089 | LR: 5.56e-05
2025-10-27 15:06:51,233 - INFO - Step 49700/100000 | Loss: 0.0102 | LR: 5.54e-05
2025-10-27 15:06:56,524 - INFO - Step 49800/100000 | Loss: 0.0099 | LR: 5.53e-05
2025-10-27 15:07:01,852 - INFO - Step 49900/100000 | Loss: 0.0099 | LR: 5.51e-05
2025-10-27 15:07:07,154 - INFO - Step 50000/100000 | Loss: 0.0108 | LR: 5.50e-05
2025-10-27 15:07:07,155 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_50000
2025-10-27 15:07:14,994 - INFO - Step 50100/100000 | Loss: 0.0097 | LR: 5.49e-05
2025-10-27 15:07:20,326 - INFO - Step 50200/100000 | Loss: 0.0087 | LR: 5.47e-05
2025-10-27 15:07:25,650 - INFO - Step 50300/100000 | Loss: 0.0116 | LR: 5.46e-05
2025-10-27 15:07:30,960 - INFO - Step 50400/100000 | Loss: 0.0093 | LR: 5.44e-05
2025-10-27 15:07:36,262 - INFO - Step 50500/100000 | Loss: 0.0101 | LR: 5.43e-05
2025-10-27 15:07:41,549 - INFO - Step 50600/100000 | Loss: 0.0115 | LR: 5.42e-05
2025-10-27 15:07:46,867 - INFO - Step 50700/100000 | Loss: 0.0100 | LR: 5.40e-05
2025-10-27 15:07:52,185 - INFO - Step 50800/100000 | Loss: 0.0088 | LR: 5.39e-05
2025-10-27 15:07:57,466 - INFO - Step 50900/100000 | Loss: 0.0098 | LR: 5.37e-05
2025-10-27 15:08:02,787 - INFO - Step 51000/100000 | Loss: 0.0092 | LR: 5.36e-05
2025-10-27 15:08:08,120 - INFO - Step 51100/100000 | Loss: 0.0104 | LR: 5.34e-05
2025-10-27 15:08:13,450 - INFO - Step 51200/100000 | Loss: 0.0097 | LR: 5.33e-05
2025-10-27 15:08:18,782 - INFO - Step 51300/100000 | Loss: 0.0095 | LR: 5.32e-05
2025-10-27 15:08:24,101 - INFO - Step 51400/100000 | Loss: 0.0094 | LR: 5.30e-05
2025-10-27 15:08:29,433 - INFO - Step 51500/100000 | Loss: 0.0099 | LR: 5.29e-05
2025-10-27 15:08:34,752 - INFO - Step 51600/100000 | Loss: 0.0107 | LR: 5.27e-05
2025-10-27 15:08:40,081 - INFO - Step 51700/100000 | Loss: 0.0101 | LR: 5.26e-05
2025-10-27 15:08:45,394 - INFO - Step 51800/100000 | Loss: 0.0103 | LR: 5.25e-05
2025-10-27 15:08:50,722 - INFO - Step 51900/100000 | Loss: 0.0108 | LR: 5.23e-05
2025-10-27 15:08:56,020 - INFO - Step 52000/100000 | Loss: 0.0085 | LR: 5.22e-05
2025-10-27 15:09:01,372 - INFO - Step 52100/100000 | Loss: 0.0098 | LR: 5.20e-05
2025-10-27 15:09:06,659 - INFO - Step 52200/100000 | Loss: 0.0103 | LR: 5.19e-05
2025-10-27 15:09:11,950 - INFO - Step 52300/100000 | Loss: 0.0099 | LR: 5.17e-05
2025-10-27 15:09:17,289 - INFO - Step 52400/100000 | Loss: 0.0097 | LR: 5.16e-05
2025-10-27 15:09:22,608 - INFO - Step 52500/100000 | Loss: 0.0097 | LR: 5.15e-05
2025-10-27 15:09:27,925 - INFO - Step 52600/100000 | Loss: 0.0096 | LR: 5.13e-05
2025-10-27 15:09:33,291 - INFO - Step 52700/100000 | Loss: 0.0092 | LR: 5.12e-05
2025-10-27 15:09:38,683 - INFO - Step 52800/100000 | Loss: 0.0080 | LR: 5.10e-05
2025-10-27 15:09:44,038 - INFO - Step 52900/100000 | Loss: 0.0094 | LR: 5.09e-05
2025-10-27 15:09:49,397 - INFO - Step 53000/100000 | Loss: 0.0084 | LR: 5.08e-05
2025-10-27 15:09:54,720 - INFO - Step 53100/100000 | Loss: 0.0098 | LR: 5.06e-05
2025-10-27 15:09:59,995 - INFO - Step 53200/100000 | Loss: 0.0097 | LR: 5.05e-05
2025-10-27 15:10:05,283 - INFO - Step 53300/100000 | Loss: 0.0104 | LR: 5.03e-05
2025-10-27 15:10:10,571 - INFO - Step 53400/100000 | Loss: 0.0092 | LR: 5.02e-05
2025-10-27 15:10:15,905 - INFO - Step 53500/100000 | Loss: 0.0099 | LR: 5.01e-05
2025-10-27 15:10:21,207 - INFO - Step 53600/100000 | Loss: 0.0104 | LR: 4.99e-05
2025-10-27 15:10:26,532 - INFO - Step 53700/100000 | Loss: 0.0111 | LR: 4.98e-05
2025-10-27 15:10:31,854 - INFO - Step 53800/100000 | Loss: 0.0086 | LR: 4.96e-05
2025-10-27 15:10:37,194 - INFO - Step 53900/100000 | Loss: 0.0103 | LR: 4.95e-05
2025-10-27 15:10:42,495 - INFO - Step 54000/100000 | Loss: 0.0087 | LR: 4.94e-05
2025-10-27 15:10:47,787 - INFO - Step 54100/100000 | Loss: 0.0081 | LR: 4.92e-05
2025-10-27 15:10:53,124 - INFO - Step 54200/100000 | Loss: 0.0087 | LR: 4.91e-05
2025-10-27 15:10:58,409 - INFO - Step 54300/100000 | Loss: 0.0092 | LR: 4.89e-05
2025-10-27 15:11:03,693 - INFO - Step 54400/100000 | Loss: 0.0083 | LR: 4.88e-05
2025-10-27 15:11:09,012 - INFO - Step 54500/100000 | Loss: 0.0088 | LR: 4.87e-05
2025-10-27 15:11:14,346 - INFO - Step 54600/100000 | Loss: 0.0093 | LR: 4.85e-05
2025-10-27 15:11:19,657 - INFO - Step 54700/100000 | Loss: 0.0093 | LR: 4.84e-05
2025-10-27 15:11:24,963 - INFO - Step 54800/100000 | Loss: 0.0093 | LR: 4.82e-05
2025-10-27 15:11:30,291 - INFO - Step 54900/100000 | Loss: 0.0080 | LR: 4.81e-05
2025-10-27 15:11:35,601 - INFO - Step 55000/100000 | Loss: 0.0088 | LR: 4.80e-05
2025-10-27 15:11:40,912 - INFO - Step 55100/100000 | Loss: 0.0084 | LR: 4.78e-05
2025-10-27 15:11:46,206 - INFO - Step 55200/100000 | Loss: 0.0104 | LR: 4.77e-05
2025-10-27 15:11:51,531 - INFO - Step 55300/100000 | Loss: 0.0114 | LR: 4.75e-05
2025-10-27 15:11:56,882 - INFO - Step 55400/100000 | Loss: 0.0095 | LR: 4.74e-05
2025-10-27 15:12:02,229 - INFO - Step 55500/100000 | Loss: 0.0086 | LR: 4.73e-05
2025-10-27 15:12:07,587 - INFO - Step 55600/100000 | Loss: 0.0095 | LR: 4.71e-05
2025-10-27 15:12:12,925 - INFO - Step 55700/100000 | Loss: 0.0090 | LR: 4.70e-05
2025-10-27 15:12:18,247 - INFO - Step 55800/100000 | Loss: 0.0099 | LR: 4.68e-05
2025-10-27 15:12:23,560 - INFO - Step 55900/100000 | Loss: 0.0094 | LR: 4.67e-05
2025-10-27 15:12:28,869 - INFO - Step 56000/100000 | Loss: 0.0095 | LR: 4.66e-05
2025-10-27 15:12:34,206 - INFO - Step 56100/100000 | Loss: 0.0091 | LR: 4.64e-05
2025-10-27 15:12:39,550 - INFO - Step 56200/100000 | Loss: 0.0092 | LR: 4.63e-05
2025-10-27 15:12:44,865 - INFO - Step 56300/100000 | Loss: 0.0110 | LR: 4.62e-05
2025-10-27 15:12:50,167 - INFO - Step 56400/100000 | Loss: 0.0088 | LR: 4.60e-05
2025-10-27 15:12:55,478 - INFO - Step 56500/100000 | Loss: 0.0092 | LR: 4.59e-05
2025-10-27 15:13:00,766 - INFO - Step 56600/100000 | Loss: 0.0097 | LR: 4.57e-05
2025-10-27 15:13:06,057 - INFO - Step 56700/100000 | Loss: 0.0095 | LR: 4.56e-05
2025-10-27 15:13:11,341 - INFO - Step 56800/100000 | Loss: 0.0100 | LR: 4.55e-05
2025-10-27 15:13:16,626 - INFO - Step 56900/100000 | Loss: 0.0097 | LR: 4.53e-05
2025-10-27 15:13:21,928 - INFO - Step 57000/100000 | Loss: 0.0097 | LR: 4.52e-05
2025-10-27 15:13:27,283 - INFO - Step 57100/100000 | Loss: 0.0092 | LR: 4.50e-05
2025-10-27 15:13:32,677 - INFO - Step 57200/100000 | Loss: 0.0097 | LR: 4.49e-05
2025-10-27 15:13:38,064 - INFO - Step 57300/100000 | Loss: 0.0098 | LR: 4.48e-05
2025-10-27 15:13:43,448 - INFO - Step 57400/100000 | Loss: 0.0086 | LR: 4.46e-05
2025-10-27 15:13:48,814 - INFO - Step 57500/100000 | Loss: 0.0104 | LR: 4.45e-05
2025-10-27 15:13:54,149 - INFO - Step 57600/100000 | Loss: 0.0107 | LR: 4.44e-05
2025-10-27 15:13:59,467 - INFO - Step 57700/100000 | Loss: 0.0086 | LR: 4.42e-05
2025-10-27 15:14:04,760 - INFO - Step 57800/100000 | Loss: 0.0082 | LR: 4.41e-05
2025-10-27 15:14:10,044 - INFO - Step 57900/100000 | Loss: 0.0096 | LR: 4.39e-05
2025-10-27 15:14:15,375 - INFO - Step 58000/100000 | Loss: 0.0085 | LR: 4.38e-05
2025-10-27 15:14:20,753 - INFO - Step 58100/100000 | Loss: 0.0104 | LR: 4.37e-05
2025-10-27 15:14:26,132 - INFO - Step 58200/100000 | Loss: 0.0078 | LR: 4.35e-05
2025-10-27 15:14:31,478 - INFO - Step 58300/100000 | Loss: 0.0074 | LR: 4.34e-05
2025-10-27 15:14:36,760 - INFO - Step 58400/100000 | Loss: 0.0094 | LR: 4.33e-05
2025-10-27 15:14:37,750 - INFO - Completed epoch 5
2025-10-27 15:14:42,278 - INFO - Step 58500/100000 | Loss: 0.0090 | LR: 4.31e-05
2025-10-27 15:14:47,591 - INFO - Step 58600/100000 | Loss: 0.0091 | LR: 4.30e-05
2025-10-27 15:14:52,888 - INFO - Step 58700/100000 | Loss: 0.0089 | LR: 4.29e-05
2025-10-27 15:14:58,238 - INFO - Step 58800/100000 | Loss: 0.0093 | LR: 4.27e-05
2025-10-27 15:15:03,583 - INFO - Step 58900/100000 | Loss: 0.0092 | LR: 4.26e-05
2025-10-27 15:15:08,827 - INFO - Step 59000/100000 | Loss: 0.0092 | LR: 4.24e-05
2025-10-27 15:15:14,130 - INFO - Step 59100/100000 | Loss: 0.0092 | LR: 4.23e-05
2025-10-27 15:15:19,458 - INFO - Step 59200/100000 | Loss: 0.0091 | LR: 4.22e-05
2025-10-27 15:15:24,749 - INFO - Step 59300/100000 | Loss: 0.0082 | LR: 4.20e-05
2025-10-27 15:15:30,047 - INFO - Step 59400/100000 | Loss: 0.0091 | LR: 4.19e-05
2025-10-27 15:15:35,332 - INFO - Step 59500/100000 | Loss: 0.0091 | LR: 4.18e-05
2025-10-27 15:15:40,639 - INFO - Step 59600/100000 | Loss: 0.0090 | LR: 4.16e-05
2025-10-27 15:15:45,960 - INFO - Step 59700/100000 | Loss: 0.0088 | LR: 4.15e-05
2025-10-27 15:15:51,293 - INFO - Step 59800/100000 | Loss: 0.0090 | LR: 4.14e-05
2025-10-27 15:15:56,637 - INFO - Step 59900/100000 | Loss: 0.0076 | LR: 4.12e-05
2025-10-27 15:16:01,944 - INFO - Step 60000/100000 | Loss: 0.0080 | LR: 4.11e-05
2025-10-27 15:16:01,945 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_60000
2025-10-27 15:16:09,554 - INFO - Step 60100/100000 | Loss: 0.0079 | LR: 4.10e-05
2025-10-27 15:16:14,879 - INFO - Step 60200/100000 | Loss: 0.0108 | LR: 4.08e-05
2025-10-27 15:16:20,204 - INFO - Step 60300/100000 | Loss: 0.0094 | LR: 4.07e-05
2025-10-27 15:16:25,529 - INFO - Step 60400/100000 | Loss: 0.0084 | LR: 4.06e-05
2025-10-27 15:16:30,839 - INFO - Step 60500/100000 | Loss: 0.0088 | LR: 4.04e-05
2025-10-27 15:16:36,158 - INFO - Step 60600/100000 | Loss: 0.0089 | LR: 4.03e-05
2025-10-27 15:16:41,509 - INFO - Step 60700/100000 | Loss: 0.0082 | LR: 4.02e-05
2025-10-27 15:16:46,812 - INFO - Step 60800/100000 | Loss: 0.0085 | LR: 4.00e-05
2025-10-27 15:16:52,114 - INFO - Step 60900/100000 | Loss: 0.0088 | LR: 3.99e-05
2025-10-27 15:16:57,420 - INFO - Step 61000/100000 | Loss: 0.0091 | LR: 3.98e-05
2025-10-27 15:17:02,714 - INFO - Step 61100/100000 | Loss: 0.0079 | LR: 3.96e-05
2025-10-27 15:17:08,018 - INFO - Step 61200/100000 | Loss: 0.0091 | LR: 3.95e-05
2025-10-27 15:17:13,342 - INFO - Step 61300/100000 | Loss: 0.0081 | LR: 3.94e-05
2025-10-27 15:17:18,682 - INFO - Step 61400/100000 | Loss: 0.0088 | LR: 3.92e-05
2025-10-27 15:17:23,986 - INFO - Step 61500/100000 | Loss: 0.0098 | LR: 3.91e-05
2025-10-27 15:17:29,310 - INFO - Step 61600/100000 | Loss: 0.0088 | LR: 3.90e-05
2025-10-27 15:17:34,606 - INFO - Step 61700/100000 | Loss: 0.0075 | LR: 3.88e-05
2025-10-27 15:17:39,929 - INFO - Step 61800/100000 | Loss: 0.0084 | LR: 3.87e-05
2025-10-27 15:17:45,224 - INFO - Step 61900/100000 | Loss: 0.0085 | LR: 3.86e-05
2025-10-27 15:17:50,586 - INFO - Step 62000/100000 | Loss: 0.0084 | LR: 3.84e-05
2025-10-27 15:17:55,900 - INFO - Step 62100/100000 | Loss: 0.0089 | LR: 3.83e-05
2025-10-27 15:18:01,223 - INFO - Step 62200/100000 | Loss: 0.0094 | LR: 3.82e-05
2025-10-27 15:18:06,544 - INFO - Step 62300/100000 | Loss: 0.0083 | LR: 3.80e-05
2025-10-27 15:18:11,863 - INFO - Step 62400/100000 | Loss: 0.0071 | LR: 3.79e-05
2025-10-27 15:18:17,206 - INFO - Step 62500/100000 | Loss: 0.0091 | LR: 3.78e-05
2025-10-27 15:18:22,545 - INFO - Step 62600/100000 | Loss: 0.0085 | LR: 3.76e-05
2025-10-27 15:18:27,889 - INFO - Step 62700/100000 | Loss: 0.0089 | LR: 3.75e-05
2025-10-27 15:18:33,212 - INFO - Step 62800/100000 | Loss: 0.0080 | LR: 3.74e-05
2025-10-27 15:18:38,547 - INFO - Step 62900/100000 | Loss: 0.0086 | LR: 3.73e-05
2025-10-27 15:18:43,852 - INFO - Step 63000/100000 | Loss: 0.0098 | LR: 3.71e-05
2025-10-27 15:18:49,196 - INFO - Step 63100/100000 | Loss: 0.0076 | LR: 3.70e-05
2025-10-27 15:18:54,495 - INFO - Step 63200/100000 | Loss: 0.0086 | LR: 3.69e-05
2025-10-27 15:18:59,769 - INFO - Step 63300/100000 | Loss: 0.0093 | LR: 3.67e-05
2025-10-27 15:19:05,091 - INFO - Step 63400/100000 | Loss: 0.0090 | LR: 3.66e-05
2025-10-27 15:19:10,422 - INFO - Step 63500/100000 | Loss: 0.0082 | LR: 3.65e-05
2025-10-27 15:19:15,730 - INFO - Step 63600/100000 | Loss: 0.0082 | LR: 3.64e-05
2025-10-27 15:19:21,053 - INFO - Step 63700/100000 | Loss: 0.0066 | LR: 3.62e-05
2025-10-27 15:19:26,351 - INFO - Step 63800/100000 | Loss: 0.0086 | LR: 3.61e-05
2025-10-27 15:19:31,643 - INFO - Step 63900/100000 | Loss: 0.0091 | LR: 3.60e-05
2025-10-27 15:19:36,983 - INFO - Step 64000/100000 | Loss: 0.0081 | LR: 3.58e-05
2025-10-27 15:19:42,294 - INFO - Step 64100/100000 | Loss: 0.0081 | LR: 3.57e-05
2025-10-27 15:19:47,623 - INFO - Step 64200/100000 | Loss: 0.0083 | LR: 3.56e-05
2025-10-27 15:19:52,945 - INFO - Step 64300/100000 | Loss: 0.0085 | LR: 3.55e-05
2025-10-27 15:19:58,241 - INFO - Step 64400/100000 | Loss: 0.0097 | LR: 3.53e-05
2025-10-27 15:20:03,545 - INFO - Step 64500/100000 | Loss: 0.0078 | LR: 3.52e-05
2025-10-27 15:20:08,811 - INFO - Step 64600/100000 | Loss: 0.0092 | LR: 3.51e-05
2025-10-27 15:20:14,075 - INFO - Step 64700/100000 | Loss: 0.0091 | LR: 3.49e-05
2025-10-27 15:20:19,386 - INFO - Step 64800/100000 | Loss: 0.0094 | LR: 3.48e-05
2025-10-27 15:20:24,664 - INFO - Step 64900/100000 | Loss: 0.0078 | LR: 3.47e-05
2025-10-27 15:20:29,974 - INFO - Step 65000/100000 | Loss: 0.0083 | LR: 3.46e-05
2025-10-27 15:20:35,305 - INFO - Step 65100/100000 | Loss: 0.0080 | LR: 3.44e-05
2025-10-27 15:20:40,606 - INFO - Step 65200/100000 | Loss: 0.0089 | LR: 3.43e-05
2025-10-27 15:20:45,895 - INFO - Step 65300/100000 | Loss: 0.0086 | LR: 3.42e-05
2025-10-27 15:20:51,198 - INFO - Step 65400/100000 | Loss: 0.0099 | LR: 3.41e-05
2025-10-27 15:20:56,512 - INFO - Step 65500/100000 | Loss: 0.0087 | LR: 3.39e-05
2025-10-27 15:21:01,826 - INFO - Step 65600/100000 | Loss: 0.0095 | LR: 3.38e-05
2025-10-27 15:21:07,152 - INFO - Step 65700/100000 | Loss: 0.0090 | LR: 3.37e-05
2025-10-27 15:21:12,456 - INFO - Step 65800/100000 | Loss: 0.0087 | LR: 3.36e-05
2025-10-27 15:21:17,748 - INFO - Step 65900/100000 | Loss: 0.0096 | LR: 3.34e-05
2025-10-27 15:21:23,047 - INFO - Step 66000/100000 | Loss: 0.0087 | LR: 3.33e-05
2025-10-27 15:21:28,367 - INFO - Step 66100/100000 | Loss: 0.0081 | LR: 3.32e-05
2025-10-27 15:21:33,759 - INFO - Step 66200/100000 | Loss: 0.0071 | LR: 3.31e-05
2025-10-27 15:21:39,076 - INFO - Step 66300/100000 | Loss: 0.0080 | LR: 3.29e-05
2025-10-27 15:21:44,403 - INFO - Step 66400/100000 | Loss: 0.0082 | LR: 3.28e-05
2025-10-27 15:21:49,728 - INFO - Step 66500/100000 | Loss: 0.0079 | LR: 3.27e-05
2025-10-27 15:21:54,994 - INFO - Step 66600/100000 | Loss: 0.0081 | LR: 3.26e-05
2025-10-27 15:22:00,302 - INFO - Step 66700/100000 | Loss: 0.0086 | LR: 3.25e-05
2025-10-27 15:22:05,591 - INFO - Step 66800/100000 | Loss: 0.0075 | LR: 3.23e-05
2025-10-27 15:22:10,876 - INFO - Step 66900/100000 | Loss: 0.0070 | LR: 3.22e-05
2025-10-27 15:22:16,220 - INFO - Step 67000/100000 | Loss: 0.0081 | LR: 3.21e-05
2025-10-27 15:22:21,501 - INFO - Step 67100/100000 | Loss: 0.0084 | LR: 3.20e-05
2025-10-27 15:22:26,818 - INFO - Step 67200/100000 | Loss: 0.0091 | LR: 3.18e-05
2025-10-27 15:22:32,148 - INFO - Step 67300/100000 | Loss: 0.0088 | LR: 3.17e-05
2025-10-27 15:22:37,419 - INFO - Step 67400/100000 | Loss: 0.0100 | LR: 3.16e-05
2025-10-27 15:22:42,684 - INFO - Step 67500/100000 | Loss: 0.0086 | LR: 3.15e-05
2025-10-27 15:22:48,007 - INFO - Step 67600/100000 | Loss: 0.0080 | LR: 3.14e-05
2025-10-27 15:22:53,281 - INFO - Step 67700/100000 | Loss: 0.0084 | LR: 3.12e-05
2025-10-27 15:22:58,573 - INFO - Step 67800/100000 | Loss: 0.0086 | LR: 3.11e-05
2025-10-27 15:23:03,874 - INFO - Step 67900/100000 | Loss: 0.0075 | LR: 3.10e-05
2025-10-27 15:23:09,137 - INFO - Step 68000/100000 | Loss: 0.0086 | LR: 3.09e-05
2025-10-27 15:23:14,417 - INFO - Step 68100/100000 | Loss: 0.0077 | LR: 3.08e-05
2025-10-27 15:23:19,674 - INFO - Step 68200/100000 | Loss: 0.0077 | LR: 3.06e-05
2025-10-27 15:23:24,986 - INFO - Step 68300/100000 | Loss: 0.0093 | LR: 3.05e-05
2025-10-27 15:23:30,295 - INFO - Step 68400/100000 | Loss: 0.0070 | LR: 3.04e-05
2025-10-27 15:23:35,606 - INFO - Step 68500/100000 | Loss: 0.0088 | LR: 3.03e-05
2025-10-27 15:23:40,891 - INFO - Step 68600/100000 | Loss: 0.0077 | LR: 3.02e-05
2025-10-27 15:23:46,149 - INFO - Step 68700/100000 | Loss: 0.0082 | LR: 3.01e-05
2025-10-27 15:23:51,450 - INFO - Step 68800/100000 | Loss: 0.0091 | LR: 2.99e-05
2025-10-27 15:23:56,728 - INFO - Step 68900/100000 | Loss: 0.0073 | LR: 2.98e-05
2025-10-27 15:24:02,005 - INFO - Step 69000/100000 | Loss: 0.0087 | LR: 2.97e-05
2025-10-27 15:24:07,311 - INFO - Step 69100/100000 | Loss: 0.0086 | LR: 2.96e-05
2025-10-27 15:24:12,615 - INFO - Step 69200/100000 | Loss: 0.0069 | LR: 2.95e-05
2025-10-27 15:24:17,926 - INFO - Step 69300/100000 | Loss: 0.0076 | LR: 2.94e-05
2025-10-27 15:24:23,217 - INFO - Step 69400/100000 | Loss: 0.0072 | LR: 2.92e-05
2025-10-27 15:24:28,505 - INFO - Step 69500/100000 | Loss: 0.0087 | LR: 2.91e-05
2025-10-27 15:24:33,805 - INFO - Step 69600/100000 | Loss: 0.0082 | LR: 2.90e-05
2025-10-27 15:24:39,102 - INFO - Step 69700/100000 | Loss: 0.0093 | LR: 2.89e-05
2025-10-27 15:24:44,425 - INFO - Step 69800/100000 | Loss: 0.0080 | LR: 2.88e-05
2025-10-27 15:24:49,743 - INFO - Step 69900/100000 | Loss: 0.0080 | LR: 2.87e-05
2025-10-27 15:24:55,061 - INFO - Step 70000/100000 | Loss: 0.0075 | LR: 2.85e-05
2025-10-27 15:24:55,062 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_70000
2025-10-27 15:24:57,397 - INFO - New best loss: 0.0038, saving to outputs/train/g1_diffusion/checkpoints/best
2025-10-27 15:25:04,053 - INFO - Completed epoch 6
2025-10-27 15:25:04,225 - INFO - Step 70100/100000 | Loss: 0.0084 | LR: 2.84e-05
2025-10-27 15:25:09,515 - INFO - Step 70200/100000 | Loss: 0.0086 | LR: 2.83e-05
2025-10-27 15:25:14,800 - INFO - Step 70300/100000 | Loss: 0.0081 | LR: 2.82e-05
2025-10-27 15:25:20,091 - INFO - Step 70400/100000 | Loss: 0.0075 | LR: 2.81e-05
2025-10-27 15:25:25,368 - INFO - Step 70500/100000 | Loss: 0.0084 | LR: 2.80e-05
2025-10-27 15:25:30,691 - INFO - Step 70600/100000 | Loss: 0.0071 | LR: 2.79e-05
2025-10-27 15:25:35,995 - INFO - Step 70700/100000 | Loss: 0.0072 | LR: 2.78e-05
2025-10-27 15:25:41,300 - INFO - Step 70800/100000 | Loss: 0.0078 | LR: 2.76e-05
2025-10-27 15:25:46,592 - INFO - Step 70900/100000 | Loss: 0.0077 | LR: 2.75e-05
2025-10-27 15:25:51,890 - INFO - Step 71000/100000 | Loss: 0.0081 | LR: 2.74e-05
2025-10-27 15:25:57,169 - INFO - Step 71100/100000 | Loss: 0.0079 | LR: 2.73e-05
2025-10-27 15:26:02,445 - INFO - Step 71200/100000 | Loss: 0.0080 | LR: 2.72e-05
2025-10-27 15:26:07,744 - INFO - Step 71300/100000 | Loss: 0.0082 | LR: 2.71e-05
2025-10-27 15:26:13,030 - INFO - Step 71400/100000 | Loss: 0.0081 | LR: 2.70e-05
2025-10-27 15:26:18,309 - INFO - Step 71500/100000 | Loss: 0.0080 | LR: 2.69e-05
2025-10-27 15:26:23,586 - INFO - Step 71600/100000 | Loss: 0.0068 | LR: 2.68e-05
2025-10-27 15:26:28,885 - INFO - Step 71700/100000 | Loss: 0.0082 | LR: 2.66e-05
2025-10-27 15:26:34,202 - INFO - Step 71800/100000 | Loss: 0.0069 | LR: 2.65e-05
2025-10-27 15:26:39,492 - INFO - Step 71900/100000 | Loss: 0.0091 | LR: 2.64e-05
2025-10-27 15:26:44,776 - INFO - Step 72000/100000 | Loss: 0.0076 | LR: 2.63e-05
2025-10-27 15:26:50,069 - INFO - Step 72100/100000 | Loss: 0.0080 | LR: 2.62e-05
2025-10-27 15:26:55,374 - INFO - Step 72200/100000 | Loss: 0.0091 | LR: 2.61e-05
2025-10-27 15:27:00,665 - INFO - Step 72300/100000 | Loss: 0.0074 | LR: 2.60e-05
2025-10-27 15:27:05,944 - INFO - Step 72400/100000 | Loss: 0.0080 | LR: 2.59e-05
2025-10-27 15:27:11,294 - INFO - Step 72500/100000 | Loss: 0.0084 | LR: 2.58e-05
2025-10-27 15:27:16,631 - INFO - Step 72600/100000 | Loss: 0.0073 | LR: 2.57e-05
2025-10-27 15:27:21,975 - INFO - Step 72700/100000 | Loss: 0.0073 | LR: 2.56e-05
2025-10-27 15:27:27,259 - INFO - Step 72800/100000 | Loss: 0.0080 | LR: 2.55e-05
2025-10-27 15:27:32,558 - INFO - Step 72900/100000 | Loss: 0.0075 | LR: 2.53e-05
2025-10-27 15:27:37,839 - INFO - Step 73000/100000 | Loss: 0.0081 | LR: 2.52e-05
2025-10-27 15:27:43,138 - INFO - Step 73100/100000 | Loss: 0.0092 | LR: 2.51e-05
2025-10-27 15:27:48,440 - INFO - Step 73200/100000 | Loss: 0.0074 | LR: 2.50e-05
2025-10-27 15:27:53,733 - INFO - Step 73300/100000 | Loss: 0.0076 | LR: 2.49e-05
2025-10-27 15:27:59,052 - INFO - Step 73400/100000 | Loss: 0.0073 | LR: 2.48e-05
2025-10-27 15:28:04,346 - INFO - Step 73500/100000 | Loss: 0.0081 | LR: 2.47e-05
2025-10-27 15:28:09,656 - INFO - Step 73600/100000 | Loss: 0.0087 | LR: 2.46e-05
2025-10-27 15:28:14,948 - INFO - Step 73700/100000 | Loss: 0.0078 | LR: 2.45e-05
2025-10-27 15:28:20,245 - INFO - Step 73800/100000 | Loss: 0.0098 | LR: 2.44e-05
2025-10-27 15:28:25,563 - INFO - Step 73900/100000 | Loss: 0.0071 | LR: 2.43e-05
2025-10-27 15:28:30,860 - INFO - Step 74000/100000 | Loss: 0.0073 | LR: 2.42e-05
2025-10-27 15:28:36,150 - INFO - Step 74100/100000 | Loss: 0.0081 | LR: 2.41e-05
2025-10-27 15:28:41,506 - INFO - Step 74200/100000 | Loss: 0.0081 | LR: 2.40e-05
2025-10-27 15:28:46,883 - INFO - Step 74300/100000 | Loss: 0.0077 | LR: 2.39e-05
2025-10-27 15:28:52,320 - INFO - Step 74400/100000 | Loss: 0.0083 | LR: 2.38e-05
2025-10-27 15:28:57,716 - INFO - Step 74500/100000 | Loss: 0.0077 | LR: 2.37e-05
2025-10-27 15:29:03,147 - INFO - Step 74600/100000 | Loss: 0.0065 | LR: 2.36e-05
2025-10-27 15:29:08,513 - INFO - Step 74700/100000 | Loss: 0.0074 | LR: 2.35e-05
2025-10-27 15:29:13,856 - INFO - Step 74800/100000 | Loss: 0.0073 | LR: 2.34e-05
2025-10-27 15:29:19,127 - INFO - Step 74900/100000 | Loss: 0.0078 | LR: 2.33e-05
2025-10-27 15:29:24,437 - INFO - Step 75000/100000 | Loss: 0.0082 | LR: 2.32e-05
2025-10-27 15:29:29,754 - INFO - Step 75100/100000 | Loss: 0.0081 | LR: 2.31e-05
2025-10-27 15:29:35,075 - INFO - Step 75200/100000 | Loss: 0.0075 | LR: 2.30e-05
2025-10-27 15:29:40,589 - INFO - Step 75300/100000 | Loss: 0.0070 | LR: 2.29e-05
2025-10-27 15:29:46,209 - INFO - Step 75400/100000 | Loss: 0.0079 | LR: 2.28e-05
2025-10-27 15:29:51,734 - INFO - Step 75500/100000 | Loss: 0.0086 | LR: 2.27e-05
2025-10-27 15:29:57,174 - INFO - Step 75600/100000 | Loss: 0.0077 | LR: 2.26e-05
2025-10-27 15:30:02,570 - INFO - Step 75700/100000 | Loss: 0.0074 | LR: 2.25e-05
2025-10-27 15:30:07,993 - INFO - Step 75800/100000 | Loss: 0.0088 | LR: 2.24e-05
2025-10-27 15:30:13,397 - INFO - Step 75900/100000 | Loss: 0.0073 | LR: 2.23e-05
2025-10-27 15:30:18,778 - INFO - Step 76000/100000 | Loss: 0.0074 | LR: 2.22e-05
2025-10-27 15:30:24,180 - INFO - Step 76100/100000 | Loss: 0.0081 | LR: 2.21e-05
2025-10-27 15:30:29,591 - INFO - Step 76200/100000 | Loss: 0.0081 | LR: 2.20e-05
2025-10-27 15:30:34,999 - INFO - Step 76300/100000 | Loss: 0.0080 | LR: 2.19e-05
2025-10-27 15:30:40,392 - INFO - Step 76400/100000 | Loss: 0.0082 | LR: 2.18e-05
2025-10-27 15:30:45,748 - INFO - Step 76500/100000 | Loss: 0.0085 | LR: 2.17e-05
2025-10-27 15:30:51,170 - INFO - Step 76600/100000 | Loss: 0.0072 | LR: 2.16e-05
2025-10-27 15:30:56,550 - INFO - Step 76700/100000 | Loss: 0.0085 | LR: 2.15e-05
2025-10-27 15:31:01,935 - INFO - Step 76800/100000 | Loss: 0.0073 | LR: 2.14e-05
2025-10-27 15:31:07,321 - INFO - Step 76900/100000 | Loss: 0.0071 | LR: 2.13e-05
2025-10-27 15:31:12,694 - INFO - Step 77000/100000 | Loss: 0.0072 | LR: 2.12e-05
2025-10-27 15:31:18,084 - INFO - Step 77100/100000 | Loss: 0.0085 | LR: 2.12e-05
2025-10-27 15:31:23,464 - INFO - Step 77200/100000 | Loss: 0.0077 | LR: 2.11e-05
2025-10-27 15:31:28,832 - INFO - Step 77300/100000 | Loss: 0.0074 | LR: 2.10e-05
2025-10-27 15:31:34,232 - INFO - Step 77400/100000 | Loss: 0.0076 | LR: 2.09e-05
2025-10-27 15:31:39,630 - INFO - Step 77500/100000 | Loss: 0.0075 | LR: 2.08e-05
2025-10-27 15:31:45,011 - INFO - Step 77600/100000 | Loss: 0.0077 | LR: 2.07e-05
2025-10-27 15:31:50,386 - INFO - Step 77700/100000 | Loss: 0.0073 | LR: 2.06e-05
2025-10-27 15:31:55,769 - INFO - Step 77800/100000 | Loss: 0.0080 | LR: 2.05e-05
2025-10-27 15:32:01,157 - INFO - Step 77900/100000 | Loss: 0.0074 | LR: 2.04e-05
2025-10-27 15:32:06,561 - INFO - Step 78000/100000 | Loss: 0.0082 | LR: 2.03e-05
2025-10-27 15:32:11,929 - INFO - Step 78100/100000 | Loss: 0.0061 | LR: 2.02e-05
2025-10-27 15:32:17,330 - INFO - Step 78200/100000 | Loss: 0.0073 | LR: 2.01e-05
2025-10-27 15:32:22,710 - INFO - Step 78300/100000 | Loss: 0.0076 | LR: 2.01e-05
2025-10-27 15:32:28,090 - INFO - Step 78400/100000 | Loss: 0.0068 | LR: 2.00e-05
2025-10-27 15:32:33,463 - INFO - Step 78500/100000 | Loss: 0.0079 | LR: 1.99e-05
2025-10-27 15:32:38,848 - INFO - Step 78600/100000 | Loss: 0.0080 | LR: 1.98e-05
2025-10-27 15:32:44,244 - INFO - Step 78700/100000 | Loss: 0.0080 | LR: 1.97e-05
2025-10-27 15:32:49,623 - INFO - Step 78800/100000 | Loss: 0.0065 | LR: 1.96e-05
2025-10-27 15:32:54,994 - INFO - Step 78900/100000 | Loss: 0.0080 | LR: 1.95e-05
2025-10-27 15:33:00,386 - INFO - Step 79000/100000 | Loss: 0.0073 | LR: 1.94e-05
2025-10-27 15:33:05,773 - INFO - Step 79100/100000 | Loss: 0.0074 | LR: 1.94e-05
2025-10-27 15:33:11,173 - INFO - Step 79200/100000 | Loss: 0.0065 | LR: 1.93e-05
2025-10-27 15:33:16,537 - INFO - Step 79300/100000 | Loss: 0.0072 | LR: 1.92e-05
2025-10-27 15:33:21,826 - INFO - Step 79400/100000 | Loss: 0.0061 | LR: 1.91e-05
2025-10-27 15:33:27,209 - INFO - Step 79500/100000 | Loss: 0.0076 | LR: 1.90e-05
2025-10-27 15:33:32,597 - INFO - Step 79600/100000 | Loss: 0.0077 | LR: 1.89e-05
2025-10-27 15:33:37,968 - INFO - Step 79700/100000 | Loss: 0.0071 | LR: 1.88e-05
2025-10-27 15:33:43,345 - INFO - Step 79800/100000 | Loss: 0.0076 | LR: 1.88e-05
2025-10-27 15:33:48,743 - INFO - Step 79900/100000 | Loss: 0.0081 | LR: 1.87e-05
2025-10-27 15:33:54,118 - INFO - Step 80000/100000 | Loss: 0.0082 | LR: 1.86e-05
2025-10-27 15:33:54,119 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_80000
2025-10-27 15:34:01,798 - INFO - Step 80100/100000 | Loss: 0.0072 | LR: 1.85e-05
2025-10-27 15:34:07,184 - INFO - Step 80200/100000 | Loss: 0.0081 | LR: 1.84e-05
2025-10-27 15:34:12,591 - INFO - Step 80300/100000 | Loss: 0.0078 | LR: 1.83e-05
2025-10-27 15:34:17,949 - INFO - Step 80400/100000 | Loss: 0.0079 | LR: 1.83e-05
2025-10-27 15:34:23,341 - INFO - Step 80500/100000 | Loss: 0.0072 | LR: 1.82e-05
2025-10-27 15:34:28,736 - INFO - Step 80600/100000 | Loss: 0.0084 | LR: 1.81e-05
2025-10-27 15:34:34,147 - INFO - Step 80700/100000 | Loss: 0.0074 | LR: 1.80e-05
2025-10-27 15:34:39,537 - INFO - Step 80800/100000 | Loss: 0.0071 | LR: 1.79e-05
2025-10-27 15:34:44,927 - INFO - Step 80900/100000 | Loss: 0.0063 | LR: 1.79e-05
2025-10-27 15:34:50,321 - INFO - Step 81000/100000 | Loss: 0.0087 | LR: 1.78e-05
2025-10-27 15:34:55,684 - INFO - Step 81100/100000 | Loss: 0.0083 | LR: 1.77e-05
2025-10-27 15:35:01,072 - INFO - Step 81200/100000 | Loss: 0.0080 | LR: 1.76e-05
2025-10-27 15:35:06,467 - INFO - Step 81300/100000 | Loss: 0.0085 | LR: 1.75e-05
2025-10-27 15:35:11,871 - INFO - Step 81400/100000 | Loss: 0.0087 | LR: 1.75e-05
2025-10-27 15:35:17,307 - INFO - Step 81500/100000 | Loss: 0.0059 | LR: 1.74e-05
2025-10-27 15:35:22,699 - INFO - Step 81600/100000 | Loss: 0.0085 | LR: 1.73e-05
2025-10-27 15:35:28,082 - INFO - Step 81700/100000 | Loss: 0.0083 | LR: 1.72e-05
2025-10-27 15:35:32,642 - INFO - Completed epoch 7
2025-10-27 15:35:33,749 - INFO - Step 81800/100000 | Loss: 0.0072 | LR: 1.72e-05
2025-10-27 15:35:39,128 - INFO - Step 81900/100000 | Loss: 0.0066 | LR: 1.71e-05
2025-10-27 15:35:44,559 - INFO - Step 82000/100000 | Loss: 0.0068 | LR: 1.70e-05
2025-10-27 15:35:49,963 - INFO - Step 82100/100000 | Loss: 0.0065 | LR: 1.69e-05
2025-10-27 15:35:55,363 - INFO - Step 82200/100000 | Loss: 0.0079 | LR: 1.69e-05
2025-10-27 15:36:00,749 - INFO - Step 82300/100000 | Loss: 0.0075 | LR: 1.68e-05
2025-10-27 15:36:06,136 - INFO - Step 82400/100000 | Loss: 0.0068 | LR: 1.67e-05
2025-10-27 15:36:11,540 - INFO - Step 82500/100000 | Loss: 0.0077 | LR: 1.66e-05
2025-10-27 15:36:16,952 - INFO - Step 82600/100000 | Loss: 0.0071 | LR: 1.66e-05
2025-10-27 15:36:22,377 - INFO - Step 82700/100000 | Loss: 0.0063 | LR: 1.65e-05
2025-10-27 15:36:27,806 - INFO - Step 82800/100000 | Loss: 0.0075 | LR: 1.64e-05
2025-10-27 15:36:33,187 - INFO - Step 82900/100000 | Loss: 0.0064 | LR: 1.63e-05
2025-10-27 15:36:38,566 - INFO - Step 83000/100000 | Loss: 0.0071 | LR: 1.63e-05
2025-10-27 15:36:43,970 - INFO - Step 83100/100000 | Loss: 0.0081 | LR: 1.62e-05
2025-10-27 15:36:49,360 - INFO - Step 83200/100000 | Loss: 0.0080 | LR: 1.61e-05
2025-10-27 15:36:54,749 - INFO - Step 83300/100000 | Loss: 0.0076 | LR: 1.61e-05
2025-10-27 15:37:00,190 - INFO - Step 83400/100000 | Loss: 0.0084 | LR: 1.60e-05
2025-10-27 15:37:05,557 - INFO - Step 83500/100000 | Loss: 0.0066 | LR: 1.59e-05
2025-10-27 15:37:10,936 - INFO - Step 83600/100000 | Loss: 0.0069 | LR: 1.58e-05
2025-10-27 15:37:16,298 - INFO - Step 83700/100000 | Loss: 0.0067 | LR: 1.58e-05
2025-10-27 15:37:21,692 - INFO - Step 83800/100000 | Loss: 0.0080 | LR: 1.57e-05
2025-10-27 15:37:27,106 - INFO - Step 83900/100000 | Loss: 0.0071 | LR: 1.56e-05
2025-10-27 15:37:32,500 - INFO - Step 84000/100000 | Loss: 0.0081 | LR: 1.56e-05
2025-10-27 15:37:37,895 - INFO - Step 84100/100000 | Loss: 0.0068 | LR: 1.55e-05
2025-10-27 15:37:43,256 - INFO - Step 84200/100000 | Loss: 0.0072 | LR: 1.54e-05
2025-10-27 15:37:48,647 - INFO - Step 84300/100000 | Loss: 0.0069 | LR: 1.54e-05
2025-10-27 15:37:54,028 - INFO - Step 84400/100000 | Loss: 0.0079 | LR: 1.53e-05
2025-10-27 15:37:59,413 - INFO - Step 84500/100000 | Loss: 0.0082 | LR: 1.52e-05
2025-10-27 15:38:04,791 - INFO - Step 84600/100000 | Loss: 0.0075 | LR: 1.52e-05
2025-10-27 15:38:10,184 - INFO - Step 84700/100000 | Loss: 0.0071 | LR: 1.51e-05
2025-10-27 15:38:15,588 - INFO - Step 84800/100000 | Loss: 0.0072 | LR: 1.50e-05
2025-10-27 15:38:20,975 - INFO - Step 84900/100000 | Loss: 0.0073 | LR: 1.50e-05
2025-10-27 15:38:26,350 - INFO - Step 85000/100000 | Loss: 0.0061 | LR: 1.49e-05
2025-10-27 15:38:31,763 - INFO - Step 85100/100000 | Loss: 0.0072 | LR: 1.48e-05
2025-10-27 15:38:37,182 - INFO - Step 85200/100000 | Loss: 0.0072 | LR: 1.48e-05
2025-10-27 15:38:42,584 - INFO - Step 85300/100000 | Loss: 0.0074 | LR: 1.47e-05
2025-10-27 15:38:47,977 - INFO - Step 85400/100000 | Loss: 0.0065 | LR: 1.47e-05
2025-10-27 15:38:53,347 - INFO - Step 85500/100000 | Loss: 0.0057 | LR: 1.46e-05
2025-10-27 15:38:58,768 - INFO - Step 85600/100000 | Loss: 0.0080 | LR: 1.45e-05
2025-10-27 15:39:04,187 - INFO - Step 85700/100000 | Loss: 0.0073 | LR: 1.45e-05
2025-10-27 15:39:09,575 - INFO - Step 85800/100000 | Loss: 0.0063 | LR: 1.44e-05
2025-10-27 15:39:14,882 - INFO - Step 85900/100000 | Loss: 0.0071 | LR: 1.43e-05
2025-10-27 15:39:20,290 - INFO - Step 86000/100000 | Loss: 0.0065 | LR: 1.43e-05
2025-10-27 15:39:25,681 - INFO - Step 86100/100000 | Loss: 0.0068 | LR: 1.42e-05
2025-10-27 15:39:31,086 - INFO - Step 86200/100000 | Loss: 0.0073 | LR: 1.42e-05
2025-10-27 15:39:36,503 - INFO - Step 86300/100000 | Loss: 0.0077 | LR: 1.41e-05
2025-10-27 15:39:41,902 - INFO - Step 86400/100000 | Loss: 0.0069 | LR: 1.40e-05
2025-10-27 15:39:47,295 - INFO - Step 86500/100000 | Loss: 0.0066 | LR: 1.40e-05
2025-10-27 15:39:52,670 - INFO - Step 86600/100000 | Loss: 0.0064 | LR: 1.39e-05
2025-10-27 15:39:58,054 - INFO - Step 86700/100000 | Loss: 0.0072 | LR: 1.39e-05
2025-10-27 15:40:03,433 - INFO - Step 86800/100000 | Loss: 0.0070 | LR: 1.38e-05
2025-10-27 15:40:08,861 - INFO - Step 86900/100000 | Loss: 0.0076 | LR: 1.38e-05
2025-10-27 15:40:14,233 - INFO - Step 87000/100000 | Loss: 0.0072 | LR: 1.37e-05
2025-10-27 15:40:19,576 - INFO - Step 87100/100000 | Loss: 0.0076 | LR: 1.36e-05
2025-10-27 15:40:24,985 - INFO - Step 87200/100000 | Loss: 0.0066 | LR: 1.36e-05
2025-10-27 15:40:30,351 - INFO - Step 87300/100000 | Loss: 0.0072 | LR: 1.35e-05
2025-10-27 15:40:35,770 - INFO - Step 87400/100000 | Loss: 0.0066 | LR: 1.35e-05
2025-10-27 15:40:41,187 - INFO - Step 87500/100000 | Loss: 0.0068 | LR: 1.34e-05
2025-10-27 15:40:46,592 - INFO - Step 87600/100000 | Loss: 0.0076 | LR: 1.34e-05
2025-10-27 15:40:52,013 - INFO - Step 87700/100000 | Loss: 0.0070 | LR: 1.33e-05
2025-10-27 15:40:57,427 - INFO - Step 87800/100000 | Loss: 0.0068 | LR: 1.33e-05
2025-10-27 15:41:02,766 - INFO - Step 87900/100000 | Loss: 0.0073 | LR: 1.32e-05
2025-10-27 15:41:08,194 - INFO - Step 88000/100000 | Loss: 0.0079 | LR: 1.32e-05
2025-10-27 15:41:13,608 - INFO - Step 88100/100000 | Loss: 0.0063 | LR: 1.31e-05
2025-10-27 15:41:18,994 - INFO - Step 88200/100000 | Loss: 0.0069 | LR: 1.31e-05
2025-10-27 15:41:24,412 - INFO - Step 88300/100000 | Loss: 0.0069 | LR: 1.30e-05
2025-10-27 15:41:29,809 - INFO - Step 88400/100000 | Loss: 0.0075 | LR: 1.30e-05
2025-10-27 15:41:35,243 - INFO - Step 88500/100000 | Loss: 0.0070 | LR: 1.29e-05
2025-10-27 15:41:40,629 - INFO - Step 88600/100000 | Loss: 0.0071 | LR: 1.29e-05
2025-10-27 15:41:46,038 - INFO - Step 88700/100000 | Loss: 0.0066 | LR: 1.28e-05
2025-10-27 15:41:51,445 - INFO - Step 88800/100000 | Loss: 0.0076 | LR: 1.28e-05
2025-10-27 15:41:56,867 - INFO - Step 88900/100000 | Loss: 0.0068 | LR: 1.27e-05
2025-10-27 15:42:02,268 - INFO - Step 89000/100000 | Loss: 0.0070 | LR: 1.27e-05
2025-10-27 15:42:07,666 - INFO - Step 89100/100000 | Loss: 0.0074 | LR: 1.26e-05
2025-10-27 15:42:13,075 - INFO - Step 89200/100000 | Loss: 0.0072 | LR: 1.26e-05
2025-10-27 15:42:18,507 - INFO - Step 89300/100000 | Loss: 0.0075 | LR: 1.25e-05
2025-10-27 15:42:23,892 - INFO - Step 89400/100000 | Loss: 0.0074 | LR: 1.25e-05
2025-10-27 15:42:29,318 - INFO - Step 89500/100000 | Loss: 0.0063 | LR: 1.24e-05
2025-10-27 15:42:34,735 - INFO - Step 89600/100000 | Loss: 0.0076 | LR: 1.24e-05
2025-10-27 15:42:40,148 - INFO - Step 89700/100000 | Loss: 0.0074 | LR: 1.23e-05
2025-10-27 15:42:45,574 - INFO - Step 89800/100000 | Loss: 0.0079 | LR: 1.23e-05
2025-10-27 15:42:50,985 - INFO - Step 89900/100000 | Loss: 0.0075 | LR: 1.22e-05
2025-10-27 15:42:56,374 - INFO - Step 90000/100000 | Loss: 0.0066 | LR: 1.22e-05
2025-10-27 15:42:56,374 - INFO - Saving checkpoint to outputs/train/g1_diffusion/checkpoints/checkpoint_90000
2025-10-27 15:43:04,191 - INFO - Step 90100/100000 | Loss: 0.0068 | LR: 1.22e-05
2025-10-27 15:43:09,625 - INFO - Step 90200/100000 | Loss: 0.0060 | LR: 1.21e-05
2025-10-27 15:43:15,056 - INFO - Step 90300/100000 | Loss: 0.0066 | LR: 1.21e-05
2025-10-27 15:43:20,463 - INFO - Step 90400/100000 | Loss: 0.0066 | LR: 1.20e-05
2025-10-27 15:43:25,882 - INFO - Step 90500/100000 | Loss: 0.0070 | LR: 1.20e-05
2025-10-27 15:43:31,306 - INFO - Step 90600/100000 | Loss: 0.0074 | LR: 1.19e-05
2025-10-27 15:43:36,712 - INFO - Step 90700/100000 | Loss: 0.0076 | LR: 1.19e-05
2025-10-27 15:43:42,131 - INFO - Step 90800/100000 | Loss: 0.0063 | LR: 1.19e-05
2025-10-27 15:43:47,546 - INFO - Step 90900/100000 | Loss: 0.0067 | LR: 1.18e-05
2025-10-27 15:43:52,964 - INFO - Step 91000/100000 | Loss: 0.0072 | LR: 1.18e-05
2025-10-27 15:43:58,392 - INFO - Step 91100/100000 | Loss: 0.0071 | LR: 1.17e-05
2025-10-27 15:44:03,791 - INFO - Step 91200/100000 | Loss: 0.0070 | LR: 1.17e-05
2025-10-27 15:44:09,206 - INFO - Step 91300/100000 | Loss: 0.0074 | LR: 1.17e-05
2025-10-27 15:44:14,615 - INFO - Step 91400/100000 | Loss: 0.0073 | LR: 1.16e-05
2025-10-27 15:44:20,018 - INFO - Step 91500/100000 | Loss: 0.0073 | LR: 1.16e-05
2025-10-27 15:44:25,400 - INFO - Step 91600/100000 | Loss: 0.0071 | LR: 1.16e-05
2025-10-27 15:44:30,799 - INFO - Step 91700/100000 | Loss: 0.0065 | LR: 1.15e-05
2025-10-27 15:44:36,221 - INFO - Step 91800/100000 | Loss: 0.0064 | LR: 1.15e-05
2025-10-27 15:44:41,629 - INFO - Step 91900/100000 | Loss: 0.0072 | LR: 1.14e-05
2025-10-27 15:44:47,029 - INFO - Step 92000/100000 | Loss: 0.0072 | LR: 1.14e-05
2025-10-27 15:44:52,445 - INFO - Step 92100/100000 | Loss: 0.0068 | LR: 1.14e-05
2025-10-27 15:44:57,862 - INFO - Step 92200/100000 | Loss: 0.0070 | LR: 1.13e-05
2025-10-27 15:45:03,263 - INFO - Step 92300/100000 | Loss: 0.0070 | LR: 1.13e-05
2025-10-27 15:45:08,664 - INFO - Step 92400/100000 | Loss: 0.0061 | LR: 1.13e-05
2025-10-27 15:45:14,064 - INFO - Step 92500/100000 | Loss: 0.0068 | LR: 1.12e-05
2025-10-27 15:45:19,472 - INFO - Step 92600/100000 | Loss: 0.0067 | LR: 1.12e-05
2025-10-27 15:45:24,906 - INFO - Step 92700/100000 | Loss: 0.0076 | LR: 1.12e-05
2025-10-27 15:45:30,285 - INFO - Step 92800/100000 | Loss: 0.0068 | LR: 1.11e-05
2025-10-27 15:45:35,693 - INFO - Step 92900/100000 | Loss: 0.0067 | LR: 1.11e-05
2025-10-27 15:45:41,073 - INFO - Step 93000/100000 | Loss: 0.0065 | LR: 1.11e-05
2025-10-27 15:45:46,505 - INFO - Step 93100/100000 | Loss: 0.0068 | LR: 1.11e-05
2025-10-27 15:45:51,901 - INFO - Step 93200/100000 | Loss: 0.0070 | LR: 1.10e-05
2025-10-27 15:45:57,255 - INFO - Step 93300/100000 | Loss: 0.0070 | LR: 1.10e-05
2025-10-27 15:46:02,625 - INFO - Step 93400/100000 | Loss: 0.0070 | LR: 1.10e-05
2025-10-27 15:46:06,299 - INFO - Completed epoch 8
2025-10-27 15:46:08,290 - INFO - Step 93500/100000 | Loss: 0.0070 | LR: 1.09e-05
2025-10-27 15:46:13,706 - INFO - Step 93600/100000 | Loss: 0.0068 | LR: 1.09e-05
2025-10-27 15:46:19,111 - INFO - Step 93700/100000 | Loss: 0.0070 | LR: 1.09e-05
2025-10-27 15:46:24,501 - INFO - Step 93800/100000 | Loss: 0.0077 | LR: 1.09e-05
2025-10-27 15:46:29,876 - INFO - Step 93900/100000 | Loss: 0.0069 | LR: 1.08e-05
2025-10-27 15:46:35,266 - INFO - Step 94000/100000 | Loss: 0.0064 | LR: 1.08e-05
2025-10-27 15:46:40,663 - INFO - Step 94100/100000 | Loss: 0.0068 | LR: 1.08e-05
2025-10-27 15:46:46,051 - INFO - Step 94200/100000 | Loss: 0.0072 | LR: 1.07e-05
2025-10-27 15:46:51,466 - INFO - Step 94300/100000 | Loss: 0.0073 | LR: 1.07e-05
2025-10-27 15:46:56,843 - INFO - Step 94400/100000 | Loss: 0.0068 | LR: 1.07e-05
2025-10-27 15:47:02,239 - INFO - Step 94500/100000 | Loss: 0.0065 | LR: 1.07e-05
2025-10-27 15:47:07,659 - INFO - Step 94600/100000 | Loss: 0.0065 | LR: 1.06e-05
2025-10-27 15:47:13,097 - INFO - Step 94700/100000 | Loss: 0.0072 | LR: 1.06e-05
2025-10-27 15:47:18,479 - INFO - Step 94800/100000 | Loss: 0.0072 | LR: 1.06e-05
2025-10-27 15:47:23,889 - INFO - Step 94900/100000 | Loss: 0.0070 | LR: 1.06e-05
2025-10-27 15:47:29,306 - INFO - Step 95000/100000 | Loss: 0.0072 | LR: 1.06e-05
2025-10-27 15:47:34,717 - INFO - Step 95100/100000 | Loss: 0.0065 | LR: 1.05e-05
2025-10-27 15:47:40,118 - INFO - Step 95200/100000 | Loss: 0.0067 | LR: 1.05e-05
2025-10-27 15:47:45,491 - INFO - Step 95300/100000 | Loss: 0.0076 | LR: 1.05e-05
2025-10-27 15:47:50,885 - INFO - Step 95400/100000 | Loss: 0.0075 | LR: 1.05e-05
2025-10-27 15:47:56,284 - INFO - Step 95500/100000 | Loss: 0.0076 | LR: 1.04e-05
2025-10-27 15:48:01,703 - INFO - Step 95600/100000 | Loss: 0.0077 | LR: 1.04e-05
2025-10-27 15:48:07,087 - INFO - Step 95700/100000 | Loss: 0.0064 | LR: 1.04e-05
2025-10-27 15:48:12,474 - INFO - Step 95800/100000 | Loss: 0.0065 | LR: 1.04e-05
2025-10-27 15:48:17,845 - INFO - Step 95900/100000 | Loss: 0.0065 | LR: 1.04e-05
2025-10-27 15:48:23,233 - INFO - Step 96000/100000 | Loss: 0.0063 | LR: 1.04e-05
2025-10-27 15:48:28,630 - INFO - Step 96100/100000 | Loss: 0.0068 | LR: 1.03e-05
2025-10-27 15:48:34,035 - INFO - Step 96200/100000 | Loss: 0.0073 | LR: 1.03e-05
2025-10-27 15:48:39,466 - INFO - Step 96300/100000 | Loss: 0.0070 | LR: 1.03e-05
2025-10-27 15:48:44,857 - INFO - Step 96400/100000 | Loss: 0.0067 | LR: 1.03e-05
2025-10-27 15:48:50,220 - INFO - Step 96500/100000 | Loss: 0.0066 | LR: 1.03e-05
2025-10-27 15:48:55,642 - INFO - Step 96600/100000 | Loss: 0.0073 | LR: 1.03e-05
2025-10-27 15:49:01,051 - INFO - Step 96700/100000 | Loss: 0.0066 | LR: 1.02e-05
2025-10-27 15:49:06,441 - INFO - Step 96800/100000 | Loss: 0.0071 | LR: 1.02e-05
2025-10-27 15:49:11,819 - INFO - Step 96900/100000 | Loss: 0.0077 | LR: 1.02e-05
2025-10-27 15:49:17,227 - INFO - Step 97000/100000 | Loss: 0.0073 | LR: 1.02e-05
2025-10-27 15:49:22,609 - INFO - Step 97100/100000 | Loss: 0.0062 | LR: 1.02e-05
2025-10-27 15:49:28,024 - INFO - Step 97200/100000 | Loss: 0.0069 | LR: 1.02e-05
2025-10-27 15:49:33,407 - INFO - Step 97300/100000 | Loss: 0.0061 | LR: 1.02e-05
2025-10-27 15:49:38,806 - INFO - Step 97400/100000 | Loss: 0.0073 | LR: 1.01e-05
2025-10-27 15:49:44,223 - INFO - Step 97500/100000 | Loss: 0.0071 | LR: 1.01e-05
2025-10-27 15:49:49,592 - INFO - Step 97600/100000 | Loss: 0.0063 | LR: 1.01e-05
2025-10-27 15:49:54,996 - INFO - Step 97700/100000 | Loss: 0.0078 | LR: 1.01e-05
2025-10-27 15:50:00,377 - INFO - Step 97800/100000 | Loss: 0.0064 | LR: 1.01e-05
2025-10-27 15:50:05,791 - INFO - Step 97900/100000 | Loss: 0.0066 | LR: 1.01e-05
2025-10-27 15:50:11,167 - INFO - Step 98000/100000 | Loss: 0.0066 | LR: 1.01e-05
2025-10-27 15:50:16,581 - INFO - Step 98100/100000 | Loss: 0.0069 | LR: 1.01e-05
2025-10-27 15:50:21,942 - INFO - Step 98200/100000 | Loss: 0.0064 | LR: 1.01e-05
2025-10-27 15:50:27,343 - INFO - Step 98300/100000 | Loss: 0.0068 | LR: 1.01e-05
2025-10-27 15:50:32,768 - INFO - Step 98400/100000 | Loss: 0.0093 | LR: 1.01e-05
2025-10-27 15:50:38,182 - INFO - Step 98500/100000 | Loss: 0.0071 | LR: 1.00e-05
2025-10-27 15:50:43,587 - INFO - Step 98600/100000 | Loss: 0.0067 | LR: 1.00e-05
2025-10-27 15:50:49,002 - INFO - Step 98700/100000 | Loss: 0.0069 | LR: 1.00e-05
2025-10-27 15:50:54,412 - INFO - Step 98800/100000 | Loss: 0.0065 | LR: 1.00e-05
2025-10-27 15:50:59,814 - INFO - Step 98900/100000 | Loss: 0.0066 | LR: 1.00e-05
2025-10-27 15:51:05,230 - INFO - Step 99000/100000 | Loss: 0.0065 | LR: 1.00e-05
2025-10-27 15:51:10,654 - INFO - Step 99100/100000 | Loss: 0.0063 | LR: 1.00e-05
2025-10-27 15:51:16,048 - INFO - Step 99200/100000 | Loss: 0.0068 | LR: 1.00e-05
2025-10-27 15:51:21,405 - INFO - Step 99300/100000 | Loss: 0.0071 | LR: 1.00e-05
2025-10-27 15:51:26,809 - INFO - Step 99400/100000 | Loss: 0.0066 | LR: 1.00e-05
2025-10-27 15:51:32,224 - INFO - Step 99500/100000 | Loss: 0.0074 | LR: 1.00e-05
2025-10-27 15:51:37,610 - INFO - Step 99600/100000 | Loss: 0.0071 | LR: 1.00e-05
2025-10-27 15:51:43,066 - INFO - Step 99700/100000 | Loss: 0.0067 | LR: 1.00e-05
2025-10-27 15:51:48,475 - INFO - Step 99800/100000 | Loss: 0.0061 | LR: 1.00e-05
2025-10-27 15:51:53,908 - INFO - Step 99900/100000 | Loss: 0.0071 | LR: 1.00e-05
2025-10-27 15:51:59,260 - INFO - Training completed! Saving final model to outputs/train/g1_diffusion/checkpoints/final
2025-10-27 15:52:01,687 - INFO - ================================================================================
2025-10-27 15:52:01,687 - INFO - Training finished successfully!
2025-10-27 15:52:01,687 - INFO - ================================================================================
